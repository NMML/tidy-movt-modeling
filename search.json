[
  {
    "objectID": "why-tidy.html",
    "href": "why-tidy.html",
    "title": "Why Tidy Telemetry Data",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "behavior-plots.html",
    "href": "behavior-plots.html",
    "title": "Exploring Animal Behavior",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "pathroutr-cautions.html",
    "href": "pathroutr-cautions.html",
    "title": "Cautions When Using pathroutr",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "intro-to-pathroutr.html",
    "href": "intro-to-pathroutr.html",
    "title": "Introduction to the pathroutr Package",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "crawl-utils.html",
    "href": "crawl-utils.html",
    "title": "Easier crawl-ing with crawlUtils",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "path-routing-history.html",
    "href": "path-routing-history.html",
    "title": "Routing Marine Animal Paths Around Land",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "summary-tables.html",
    "href": "summary-tables.html",
    "title": "Deployment Summary Tables",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "Package Dependencies",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "history-of-crawl.html",
    "href": "history-of-crawl.html",
    "title": "History of crawl and Movement Models in R",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "troubleshoot.html",
    "href": "troubleshoot.html",
    "title": "Troubleshooting Common Errors",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "data-sources.html",
    "href": "data-sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Where and how you access the source data for your telemetry study will depend on the type of tag and vendor. Argos location data is available to users from the Argos website and many third party sites and repositories (e.g. movebank, sea-turtle.org) have API connections to ArgosWeb that can facilitate data access. Each tag manufacturer often has additional data streams included within the Argos transmission that require specific software or processing to translate. Both the Sea Mammal Research Unit (SMRU) and Wildlife Computers provide online data portals that provide users access to the location and additional sensor data.\nRegardless of how the data are retrieved, these files should be treated as read only and not edited. ArgosWeb and vendors have, typically, kept the data formats and structure consistent over time. This affords end users the ability to develop custom processing scripts without much fear the formats will change and break their scripts."
  },
  {
    "objectID": "data-sources.html#wildlife-computers-data-sources",
    "href": "data-sources.html#wildlife-computers-data-sources",
    "title": "Data Sources",
    "section": "Wildlife Computers Data Sources",
    "text": "Wildlife Computers Data Sources\nWe have the most first-hand experience with data originating from Wildlife Computers devices and data that have been processed through the Wildlife Computers Data Portal. This doesn’t mean the techniques, packages, and analysis presented require Wildlife Computers devices. Or, that most of what is presented is not applicable to other bio-logging data sources. When possible, we’ve tried to create examples and processes that are agnostic to the type of deployed device.\n\n\n\n\n\n\nThe wcUtils Package\n\n\n\nFrequent users of the Wildlife Computers Data Portal or data from their bio-loggers processed through the DAP program may find the wcutils package for R useful. This package is maintained by Josh London and provides several utility function for downloading data from the WCDP and for tidy processing of typical data files."
  },
  {
    "objectID": "key-concepts.html",
    "href": "key-concepts.html",
    "title": "Introduction to Key Concepts",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "land-polygon.html",
    "href": "land-polygon.html",
    "title": "Sourcing a Land (barrier) Polygon",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Tidy Movement Modeling with R",
    "section": "",
    "text": "Over the years, we have produced a number of online resources intended to provide insights and examples for modeling animal movement with the crawl and pathroutr packages. Some of these documents and examples have aged well; others have not. R, the tidyverse, animal movement theory, and our experience evolve quickly. What we might have proposed as the best approach 5 years ago (or, sometimes even 5 months ago) may not be the case today.\nThis website is being developed to provide a more authoritative resource that we hope to maintain more reliably than previous attempts. The bio-logging and animal movement modeling community can also play a role by helping us improve the content. Identification of errors and suggested improvements are strongly encouraged. If you find an error or have ideas for improvement, please open an issue via the GitHub repository. We will also accept pull requests for any bug fixes or enhancements."
  },
  {
    "objectID": "index.html#a-pragmatic-guide",
    "href": "index.html#a-pragmatic-guide",
    "title": "Welcome to Tidy Movement Modeling with R",
    "section": "A Pragmatic Guide",
    "text": "A Pragmatic Guide\nThe crawl package for R (and supporting friends e.g. pathroutr, momentuHMM) is designed and built with the idea that it should be accessible and useful to a research biologist with some intermediate R skills and an understanding of the basic statistical theory behind the analysis of animal movement. This website will focus on providing the user with suggested principles, workflows, and pragmatic approaches that, if followed, should make analysis with crawl more efficient and reliable.\n\nTelemetry data are collected on a wide range of species and come in a number of formats and data structures. The code and examples provided here were developed from data the authors are most familiar with. You will very likely NOT be able to just copy and paste the code and apply to your data. We suggest you focus more on understanding what the code is doing and then write your own version of the code to meet your data and your research needs."
  },
  {
    "objectID": "index.html#content-outline",
    "href": "index.html#content-outline",
    "title": "Welcome to Tidy Movement Modeling with R",
    "section": "Content Outline",
    "text": "Content Outline\nInformation on this site is organized into the following sections:\n\nWelcome\n\nintroduction to the website (this page)\nabout the authors\npackage dependencies\n\nTelemetry Data is Messy\n\ndata sources\nwhy tidy telemetry data\ntidying messy data for easy modeling\n\nData Exploration\n\ndeployment summary tables\nmapping telemetry observations\nexploring animal behavior\n\nMovement Modeling with crawl\n\nintroduction to key concepts\nhistory of crawl and mov’t models in R\npreparing model inputs\neasier crawl-ing with crawlUtils\ntroubleshooting common errors\n\nOh No! Paths Cross Land!\n\nrouting marine animal paths around land\nintroduction to the pathroutr package\nsourcing a land/barrier polygon\npathroutr harbor seal example\ncautions when using pathroutr\n\n\nIf you’ve never been here before, you are encouraged to work through the content on this site in order. Once you have an understanding of the concepts and workflows presented, we hope this site can serve as a reference you can revisit when you forget a step or get stuck in your process."
  },
  {
    "objectID": "index.html#analysis-and-coding-principles",
    "href": "index.html#analysis-and-coding-principles",
    "title": "Welcome to Tidy Movement Modeling with R",
    "section": "Analysis and Coding Principles",
    "text": "Analysis and Coding Principles\nAs with anything in science and R, there are a number of right ways to approach a problem. The workflows and principles outlined here aren’t the only way to develop animal movement models in R. However, this content has been developed after years of working with researchers and troubleshooting common issues. For most users, following this guide will prove a successful endeavor. More advanced users or those with specific needs should feel free to refer to this as a starting point but then expand to meet their need.\n\nSource Data are Read Only\nSource data should not be edited. In many cases, the financial and personal investment required to obtain these data is significant. In addition, the unique timing, location, and nature of the data cannot be replicated so every effort should be employed to preserve the integrity of source data as they were collected. All efforts should be made to also insure that source data are stored in plain text or well described open formats. This approach provides researchers confidence that data will be available and usable well into the future.\n\n\nScript Everything\nIn all likelihood, the format of any source data will not be conducive to established analytical procedures. For example, crawl cannot accept a raw data file downloaded from ArgosWeb or the Wildlife Computers Data Portal. Some amount of data processing is required. Keeping with the principles of reproducibility, all data assembly and pipelines should be scripted. Here, we rely on the R programming language for our scripts, but Python or other similar languages will also meet this need.\n\n\nDocument Along the Way\nScripting the data assembly should be combined with an effort to properly document the process. Documentation is an important component of reproducibility and, if done as part of the scripting and development process, provides an easier workflow for publishing results and data to journals and repositories. The rmarkdown, bookdown, and quarto packages provide an excellent framework for combining your scripts with documentation. This entire website is written with quarto and knitr.\n\n\nEmbrace the Tidyverse\nThe tidyverse describes a set of R packages developed around core principles of tidy data. Tidy data principles are outlined in a 2014 paper published in the Journal of Statistical Software. The key tenants of tidy data are:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nLocation data from telemetry deployments often follows this type of structure — each location estimate is a row and the columns all represent variable attributes associated with that location. Additionally, the location data are usually organized into a single table. Behavior data, however, often comes in a less structured format. Different behavior type data are sometimes mixed within a single table and column headings do not consistently describe the same variable (e.g. Bin1, Bin2, etc). There are valid reasons for tag vendors and data providers to transmit the source data in this way, but the structure is not conducive to analysis.\nThe tidyverse package is a wrapper for a number of separate packages that each contribute to the tidy’ing of data. The tidyr, dplyr, readr, and purrr packages are key components. In addition, the lubridate package (not included in the tidyverse package) is especially useful for consistent manipulation of date-time values. More information and documentation for the packages can be found at the tidyverse.org website.\n\n\nAnticipate Errors & Trap Them\nUse R’s error trapping functions (try() and tryCatch()) in areas of your code where you anticipate parsing errors or potential issues with convergence or fit. The purrr::safely() function also provides a great service."
  },
  {
    "objectID": "tidy-messy-data.html",
    "href": "tidy-messy-data.html",
    "title": "Tidy-ing Messy Data for Easy Modeling",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "map-observations.html",
    "href": "map-observations.html",
    "title": "Mapping Telemetry Observations",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "pathroutr-demo.html",
    "href": "pathroutr-demo.html",
    "title": "A pathroutr Harbor Seal Example",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "input-data.html",
    "href": "input-data.html",
    "title": "Preparing Model Inputs",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "about-authors.html",
    "href": "about-authors.html",
    "title": "About the Authors",
    "section": "",
    "text": "Drs. Josh M. London and Devin S. Johnson are researchers with the NOAA National Marine Fisheries Service. Dr. London is a wildlife biologist at the Alaska Fisheries Science Center’s Marine Mammal Laboratory in Seattle, Washington. Dr. Johnson is a mathematical statistician at the Pacific Islands Fisheries Science Center’s Protected Species Division in Honolulu, Hawaii. Dr. London has over 15 years of experience programming and deploying satellite tags on phocid seals. He has also developed various workflows for the management of telemetry data in R. Dr. Johnson is a leading statistical ecologist with expertise in the analysis of animal movement. Dr. Johnson is the lead author and developer of the R package crawl. Dr. London is the lead author of the R package pathroutr and co-developer of crawl."
  }
]