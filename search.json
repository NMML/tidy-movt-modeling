[
  {
    "objectID": "data-sources.html",
    "href": "data-sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Where and how you access the source data for your telemetry study will depend on the type of tag and vendor. Argos location data is available to users from the Argos website and many third party sites and repositories (e.g. movebank, sea-turtle.org) have API connections to ArgosWeb that can facilitate data access. Each tag manufacturer often has additional data streams included within the Argos transmission that require specific software or processing to translate. Both the Sea Mammal Research Unit (SMRU) and Wildlife Computers provide online data portals that provide users access to the location and additional sensor data.\nRegardless of how the data are retrieved, these files should be treated as read only and not edited. ArgosWeb and vendors have, typically, kept the data formats and structure consistent over time. This affords end users the ability to develop custom processing scripts without much fear the formats will change and break their scripts."
  },
  {
    "objectID": "data-sources.html#data-management",
    "href": "data-sources.html#data-management",
    "title": "Data Sources",
    "section": "Data Management",
    "text": "Data Management\nThe quantity and diversity of data generated from bio-logging studies can quickly become overwhelming especially in situations where a single device is deployed for several months and collects data from multiple on-board sensors. For some research projects, storing these data in a central relational database (e.g. PostgreSQL) can have important benefits. If storing bio-logger data in a relational database, explore one that provides native support or extensions for spatial data types and operations (e.g. PostGIS, SpatiaLite). For many, though, simply storing data as and organized collection of plain text files (e.g. *.csv) is sufficient. This approach can be further improved by converting the comma-separated files to parquet files and leaning on the the Apache arrow package for R. These parquet files provide modern and efficient file compression, a columnar-based structure, and are optimized for big data. Even if a project starts as small data, it’s often useful to design your data management with growth in mind."
  },
  {
    "objectID": "data-sources.html#wildlife-computers-data-sources",
    "href": "data-sources.html#wildlife-computers-data-sources",
    "title": "Data Sources",
    "section": "Wildlife Computers Data Sources",
    "text": "Wildlife Computers Data Sources\nWe have the most first-hand experience with data originating from Wildlife Computers devices and data that have been processed through the Wildlife Computers Data Portal. This doesn’t mean the techniques, packages, and analysis presented require Wildlife Computers devices. Or, that most of what is presented is not applicable to other bio-logging data sources. When possible, we’ve tried to create examples and processes that are agnostic to the type of deployed device.\n\n\n\n\n\n\nThe wcUtils Package\n\n\n\nFrequent users of the Wildlife Computers Data Portal or data from their bio-loggers processed through the DAP program may find the wcutils package for R useful. This package is maintained by Josh London and provides several utility function for downloading data from the WCDP and for tidy processing of typical data files."
  },
  {
    "objectID": "path-routing-history.html",
    "href": "path-routing-history.html",
    "title": "Routing Marine Animal Paths Around Land",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "outlier-locs.html",
    "href": "outlier-locs.html",
    "title": "Removing Outlier Locations",
    "section": "",
    "text": "Location estimates from bio-loggers can sometimes provide extremely erroneous locations. This is especially true for the Argos system and the error estimates provided do not provide a realistic mechanism for removal (e.g. remove all locations with an error estimate above some threshold). If these extreme outliers are not removed from the data set prior to model fitting, issues with model convergence or other errors can arise."
  },
  {
    "objectID": "outlier-locs.html#speed-distance-and-angle-filtering",
    "href": "outlier-locs.html#speed-distance-and-angle-filtering",
    "title": "Removing Outlier Locations",
    "section": "Speed, Distance, and Angle Filtering",
    "text": "Speed, Distance, and Angle Filtering\nTo identify and remove these obviously erroneous observations we will rely on a speed, distance, and angle filter from the trip package to identify locations that would require traveling speeds that exceed 3x or more what would be expected from the study species. In our case, with bearded seals, we’ll use 7.5m/second (the likely maximum sustained speed for a bearded seal is 2.5m/second).\n\nlibrary(dplyr)\nlibrary(sf)\nlibrary(trip)\n\ndat <- akbs_locs %>% \n  group_by(deploy_id)\n\ndat_tr <- trip(dat, c(\"date\",\"deploy_id\"), correct_all = FALSE)\n\nkeep <- sda(\n          dat_tr,\n          smax = 7.5\n        )\n\nakbs_locs <- akbs_locs %>% \n  ungroup() %>% \n  mutate(sda_keep = keep) %>% \n  filter(sda_keep) %>% \n  dplyr::select(-sda_keep)"
  },
  {
    "objectID": "why-tidy.html",
    "href": "why-tidy.html",
    "title": "Why Tidy Telemetry Data",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "troubleshoot.html",
    "href": "troubleshoot.html",
    "title": "Troubleshooting Common Errors",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "map-observations.html",
    "href": "map-observations.html",
    "title": "Mapping Telemetry Observations",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "summary-tables.html",
    "href": "summary-tables.html",
    "title": "Deployment Summary Tables",
    "section": "",
    "text": "Now that we’ve setup a data structure for efficient access, imported the source data into R, and converted that data into a spatial data set, it’s time to explore and see what we have to work with. This is an important step so you can recognize any problems with the data or inconsistencies that need to be further investigated.\nSummary tables are a good way of splitting large data into components of interest and learning how our data might be distributed across those components. One example might be a simple calculation of the number of location observations within each month across the individual animals. This might identify anomalies such as locations in months prior to deployment or missing data when it was expected.\nWe’ll first want to group our location records by deployment and month.\n\nlibrary(sf)\nlibrary(lubridate)\nlibrary(dplyr)\n\ndat <- akbs_locs %>% \n  sf::st_drop_geometry() %>% \n  mutate(month = lubridate::month(date)) %>% \n  group_by(deploy_id, month) %>% \n  count(name = \"num_locs\") \n\nTo create a sensible table, let’s just focus on a single deploy_id, EB2009_3000_06A1346\n\nlibrary(knitr)\n\ndat %>% \n  dplyr::filter(deploy_id == \"EB2009_3000_06A1346\") %>% \n  dplyr::arrange(month) %>% \n  knitr::kable()\n\n\n\n\ndeploy_id\nmonth\nnum_locs\n\n\n\n\nEB2009_3000_06A1346\n1\n307\n\n\nEB2009_3000_06A1346\n2\n446\n\n\nEB2009_3000_06A1346\n3\n567\n\n\nEB2009_3000_06A1346\n4\n153\n\n\nEB2009_3000_06A1346\n6\n183\n\n\nEB2009_3000_06A1346\n7\n778\n\n\nEB2009_3000_06A1346\n8\n1007\n\n\nEB2009_3000_06A1346\n9\n764\n\n\nEB2009_3000_06A1346\n10\n748\n\n\nEB2009_3000_06A1346\n11\n753\n\n\nEB2009_3000_06A1346\n12\n476\n\n\n\n\n\nOne oddity that immediately stands out is the lack of locations in May. This, however, is to be expected as these deployments started in June and stopped transmitting in April of the following year and matches expectations for battery life.\nIf we look at another deployment, EB2009_3001_06A1332, we can see that this deployment ended in March.\n\ndat %>% \n  dplyr::filter(deploy_id == \"EB2009_3001_06A1332\") %>% \n  dplyr::arrange(month) %>% \n  knitr::kable()\n\n\n\n\ndeploy_id\nmonth\nnum_locs\n\n\n\n\nEB2009_3001_06A1332\n1\n476\n\n\nEB2009_3001_06A1332\n2\n458\n\n\nEB2009_3001_06A1332\n3\n63\n\n\nEB2009_3001_06A1332\n6\n119\n\n\nEB2009_3001_06A1332\n7\n735\n\n\nEB2009_3001_06A1332\n8\n641\n\n\nEB2009_3001_06A1332\n9\n792\n\n\nEB2009_3001_06A1332\n10\n789\n\n\nEB2009_3001_06A1332\n11\n581\n\n\nEB2009_3001_06A1332\n12\n677\n\n\n\n\n\nLastly, deployment EB2011_3001_10A0552 appears to have stopped transmitting in January of the following year which is 3-4 months earlier than any of the other devices.\n\ndat %>% \n  dplyr::filter(deploy_id == \"EB2011_3001_10A0552\") %>% \n  dplyr::arrange(month) %>% \n  knitr::kable()\n\n\n\n\ndeploy_id\nmonth\nnum_locs\n\n\n\n\nEB2011_3001_10A0552\n1\n675\n\n\nEB2011_3001_10A0552\n6\n482\n\n\nEB2011_3001_10A0552\n7\n993\n\n\nEB2011_3001_10A0552\n8\n1062\n\n\nEB2011_3001_10A0552\n9\n1134\n\n\nEB2011_3001_10A0552\n10\n904\n\n\nEB2011_3001_10A0552\n11\n896\n\n\nEB2011_3001_10A0552\n12\n1019\n\n\n\n\n\nThis is not so unusual, but such an anomaly is worth investigating further to ensure there were no issues with the data processing or other important deployment metadata."
  },
  {
    "objectID": "key-concepts.html",
    "href": "key-concepts.html",
    "title": "Introduction to Key Concepts",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "input-data.html",
    "href": "input-data.html",
    "title": "Preparing Model Inputs",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "pathroutr-cautions.html",
    "href": "pathroutr-cautions.html",
    "title": "Cautions When Using pathroutr",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "tidy-messy-data.html",
    "href": "tidy-messy-data.html",
    "title": "Tidy-ing Messy Data for Easy Modeling",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Tidy Movement Modeling with R",
    "section": "",
    "text": "Over the years, we have produced a number of online resources intended to provide insights and examples for modeling animal movement with the crawl and pathroutr packages. Some of these documents and examples have aged well; others have not. R, the tidyverse, animal movement theory, and our experience evolve quickly. What we might have proposed as the best approach 5 years ago (or, sometimes even 5 months ago) may not be the case today.\nThis website is being developed to provide a more authoritative resource that we hope to maintain more reliably than previous attempts. The bio-logging and animal movement modeling community can also play a role by helping us improve the content. Identification of errors and suggested improvements are strongly encouraged. If you find an error or have ideas for improvement, please open an issue via the GitHub repository. We will also accept pull requests for any bug fixes or enhancements."
  },
  {
    "objectID": "index.html#a-pragmatic-guide",
    "href": "index.html#a-pragmatic-guide",
    "title": "Welcome to Tidy Movement Modeling with R",
    "section": "A Pragmatic Guide",
    "text": "A Pragmatic Guide\nThe crawl package for R (and supporting friends e.g. pathroutr, momentuHMM) is designed and built with the idea that it should be accessible and useful to a research biologist with some intermediate R skills and an understanding of the basic statistical theory behind the analysis of animal movement. This website will focus on providing the user with suggested principles, workflows, and pragmatic approaches that, if followed, should make analysis with crawl more efficient and reliable.\n\n\n\n\n\n\nTelemetry data are collected on a wide range of species and come in a number of formats and data structures. The code and examples provided here were developed from data the authors are most familiar with. You will very likely NOT be able to just copy and paste the code and apply to your data. We suggest you focus more on understanding what the code is doing and then write your own version of the code to meet your data and your research needs."
  },
  {
    "objectID": "index.html#content-outline",
    "href": "index.html#content-outline",
    "title": "Welcome to Tidy Movement Modeling with R",
    "section": "Content Outline",
    "text": "Content Outline\nInformation on this site is organized into the following sections:\n\nWelcome\n\nintroduction to the website (this page)\nabout the authors\npackage dependencies\n\nTelemetry Data is Messy\n\ndata sources\nwhy tidy telemetry data\ntidying messy data for easy modeling\n\nData Exploration\n\ndeployment summary tables\nmapping telemetry observations\nexploring animal behavior\n\nMovement Modeling with crawl\n\nintroduction to key concepts\nhistory of crawl and mov’t models in R\npreparing model inputs\neasier crawl-ing with crawlUtils\ntroubleshooting common errors\n\nOh No! Paths Cross Land!\n\nrouting marine animal paths around land\nintroduction to the pathroutr package\nsourcing a land/barrier polygon\npathroutr harbor seal example\ncautions when using pathroutr\n\n\nIf you’ve never been here before, you are encouraged to work through the content on this site in order. Once you have an understanding of the concepts and workflows presented, we hope this site can serve as a reference you can revisit when you forget a step or get stuck in your process."
  },
  {
    "objectID": "index.html#analysis-and-coding-principles",
    "href": "index.html#analysis-and-coding-principles",
    "title": "Welcome to Tidy Movement Modeling with R",
    "section": "Analysis and Coding Principles",
    "text": "Analysis and Coding Principles\nAs with anything in science and R, there are a number of right ways to approach a problem. The workflows and principles outlined here aren’t the only way to develop animal movement models in R. However, this content has been developed after years of working with researchers and troubleshooting common issues. For most users, following this guide will prove a successful endeavor. More advanced users or those with specific needs should feel free to refer to this as a starting point but then expand to meet their need.\n\nSource Data are Read Only\nSource data should not be edited. In many cases, the financial and personal investment required to obtain these data is significant. In addition, the unique timing, location, and nature of the data cannot be replicated so every effort should be employed to preserve the integrity of source data as they were collected. All efforts should be made to also insure that source data are stored in plain text or well described open formats. This approach provides researchers confidence that data will be available and usable well into the future.\n\n\nScript Everything\nIn all likelihood, the format of any source data will not be conducive to established analytical procedures. For example, crawl cannot accept a raw data file downloaded from ArgosWeb or the Wildlife Computers Data Portal. Some amount of data processing is required. Keeping with the principles of reproducibility, all data assembly and pipelines should be scripted. Here, we rely on the R programming language for our scripts, but Python or other similar languages will also meet this need.\n\n\nDocument Along the Way\nScripting the data assembly should be combined with an effort to properly document the process. Documentation is an important component of reproducibility and, if done as part of the scripting and development process, provides an easier workflow for publishing results and data to journals and repositories. The rmarkdown, bookdown, and quarto packages provide an excellent framework for combining your scripts with documentation. This entire website is written with quarto and knitr.\n\n\nEmbrace the Tidyverse\nThe tidyverse describes a set of R packages developed around core principles of tidy data. Tidy data principles are outlined in a 2014 paper published in the Journal of Statistical Software. The key tenants of tidy data are:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nLocation data from telemetry deployments often follows this type of structure — each location estimate is a row and the columns all represent variable attributes associated with that location. Additionally, the location data are usually organized into a single table. Behavior data, however, often comes in a less structured format. Different behavior type data are sometimes mixed within a single table and column headings do not consistently describe the same variable (e.g. Bin1, Bin2, etc). There are valid reasons for tag vendors and data providers to transmit the source data in this way, but the structure is not conducive to analysis.\nThe tidyverse package is a wrapper for a number of separate packages that each contribute to the tidy’ing of data. The tidyr, dplyr, readr, and purrr packages are key components. In addition, the lubridate package (not included in the tidyverse package) is especially useful for consistent manipulation of date-time values. More information and documentation for the packages can be found at the tidyverse.org website.\nPlease note, however, that use of the tidyverse packages are not a requirement or indication of tidy data principles. So called, base R and other approaches in the R community can support tidy data and extensive use of tidyverse packages can also result in very un-tidy data.\n\n\nAnticipate Errors & Trap Them\nUse R’s error trapping functions (try() and tryCatch()) in areas of your code where you anticipate parsing errors or potential issues with convergence or fit. The purrr::safely() function also provides a great service."
  },
  {
    "objectID": "land-polygon.html",
    "href": "land-polygon.html",
    "title": "Sourcing a Land (barrier) Polygon",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "reading-data.html",
    "href": "reading-data.html",
    "title": "Reading Source data",
    "section": "",
    "text": "Our first step will be to read our source data into R. For this example, we’ll be using data for six bearded seal deployments of the northwest coast of Alaska. The data were downloaded as comma-separated files from the Wildlife Computers Data Portal. All data have merged into a single csv file and grouped by a ‘DeployID’ unique identifier. The raw-data/akbs_locs.csv contains all of the Argos location estimates determined for each deployment as well as fastloc GPS location estimates when available. At a minimum, this file includes identifying columns such as DeployID, PTT, Date, Quality, Type, Latitude, and Longitude. If the Argos Kalman filtering approach has been enabled (which it should be for any modern tag deployment), then additional data will be found in columns that describe the error ellipses (e.g. Error Semi-major axis, Error Semi-minor axis, Error Ellipse orientation). Note, if the tag transmitted GPS/FastLoc data, this file will list those records as Type = 'FastGPS'.\nIn many situations, the original source data may include multiple _*.csv_ or other delimited files. Instead of reading each file in one by one, the purr::map_df() function can be used to cycle through each _*.csv_ files in a directory and pass them to readr::read_csv() and, then, return a merged tibble/data.frame. This was the original case for these Alaska bearded seal data and the following code was used to create the combined _*.csv_ file from several _*-Locations.csv_ files.\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(here)\n\nmy_cols <- cols(\n  DeployID = col_character(),\n  Ptt = col_integer(),\n  Instr = col_character(),\n  Date = col_datetime(\"%H:%M:%S %d-%b-%Y\"),\n  Type = col_character(),\n  Quality = col_character(),\n  Latitude = col_double(),\n  Longitude = col_double(),\n  `Error radius` = col_integer(),\n  `Error Semi-major axis` = col_integer(),\n  `Error Semi-minor axis` = col_integer(),\n  `Error Ellipse orientation` = col_integer(),\n  Offset = col_character(),\n  `Offset orientation` = col_character(),\n  `GPE MSD` = col_character(),\n  `GPE U` = col_character(),\n  Count = col_character(),\n  Comment = col_character()\n)\n\ntbl_locs <- list.files(file.path('~/Downloads/akbs_data'),\n                       recursive = TRUE, \n                       full.names = TRUE, \n                       pattern = \"*-Locations.csv\") %>% \n  purrr::map_df( ~ readr::read_csv(.x, col_types = my_cols)) %>% \n  readr::write_csv(file = here::here('raw-data','akbs_locs.csv'))\n\nThe readr package includes the read_csv() function which we will rely on to read the csv data into R. This function is very similar to read.csv() but provides some additional functionality and consistency. One important step is to specify the column types for each column in the data set. This saves an additional step post-read where we have to mutate each column type. Note, we also rely on the here package for reliable and consistent file paths. Lastly, the janitor::clean_names() function is used to provide a consistent transformation of column names (e.g. lower and snake case with no spaces)."
  },
  {
    "objectID": "reading-data.html#efficient-data-storage-with-arrow",
    "href": "reading-data.html#efficient-data-storage-with-arrow",
    "title": "Reading Source data",
    "section": "Efficient Data Storage with Arrow",
    "text": "Efficient Data Storage with Arrow\nAs previously mentioned, thoughtful data management is an important component of any movement modeling analysis. For some, a central relational database (e.g., PostgreSQL) will be a good solution. However, in many cases, an organized collection of _*.csv_ files or an in-memory database solution (e.g. SQLite, DuckDb) will be a good option. For this example, we’re going to rely on the Apache arrow package for R and a collection of parquet files as our efficient data source.\n\nlibrary(arrow)\nlibrary(fs)\ndir_out <- here::here(\"data\",\"akbs_locs_parquet\")\nfs::dir_delete(dir_out)\nfs::dir_create(dir_out)\narrow::write_dataset(akbs_locs, dir_out, partitioning = \"deploy_id\")\nrm(akbs_locs) #remove from memory\n\nIf we take a quick look at the director/file structure withing our output directory, data/akbs_locs_parquet, we can see that arrow created a separate subdirectory for each deploy_id and there’s a separate _*.parquet_ file within each of those subdirectories\n\ndir_ls(dir_out) %>% path_rel(here::here(\"data\"))\n\nakbs_locs_parquet/deploy_id=EB2009_3000_06A1346\nakbs_locs_parquet/deploy_id=EB2009_3001_06A1332\nakbs_locs_parquet/deploy_id=EB2009_3002_06A1357\nakbs_locs_parquet/deploy_id=EB2011_3000_10A0219\nakbs_locs_parquet/deploy_id=EB2011_3001_10A0552\nakbs_locs_parquet/deploy_id=EB2011_3002_10A0200\n\n\n\ndir_ls(dir_out, recurse = TRUE, glob = \"*.parquet\") %>% \n  path_rel(here::here(\"data\"))\n\nakbs_locs_parquet/deploy_id=EB2009_3000_06A1346/part-0.parquet\nakbs_locs_parquet/deploy_id=EB2009_3001_06A1332/part-0.parquet\nakbs_locs_parquet/deploy_id=EB2009_3002_06A1357/part-0.parquet\nakbs_locs_parquet/deploy_id=EB2011_3000_10A0219/part-0.parquet\nakbs_locs_parquet/deploy_id=EB2011_3001_10A0552/part-0.parquet\nakbs_locs_parquet/deploy_id=EB2011_3002_10A0200/part-0.parquet"
  },
  {
    "objectID": "reading-data.html#querying-our-data",
    "href": "reading-data.html#querying-our-data",
    "title": "Reading Source data",
    "section": "Querying Our Data",
    "text": "Querying Our Data\nNow that we have our data organized with arrow and parquet files, we can explore ways the data can be queried and used in our analysis. There are two approaches and package frameworks we can rely on for querying:\n\nThe dplyr package’s support for arrow/parquet\nDirect use of SQL syntax with duckdb’s arrow/parquet integration\n\n\nA dplyr example\nTwo common tasks we might want to perform are to calculate the number of location records by deployment and to filter our data set based on a particular date range.\nIn this first example we create an open connection with our data set using the arrow::open_dataset() function and passing our dir_out path. Then, basic dplyr functions can return a table of location counts summarized by deploy_id. Note, unlike typical dplyr usage, the collect() function is required to actually return the data set records.\n\n# open the dataset\nakbs_ds <- arrow::open_dataset(dir_out) \n\n# query the dataset to calculate number of locations by deploy_id\nakbs_ds %>% \n  count(deploy_id, name=\"n_locs\") %>% \n  arrange(deploy_id) %>% \n  collect()\n\n# A tibble: 6 × 2\n  deploy_id           n_locs\n  <chr>                <int>\n1 EB2009_3000_06A1346   6491\n2 EB2009_3001_06A1332   5660\n3 EB2009_3002_06A1357   4913\n4 EB2011_3000_10A0219   9698\n5 EB2011_3001_10A0552   7559\n6 EB2011_3002_10A0200  10197\n\n\nIn this situation, we want to just look at locations from the months of August, September, and October. For this, the month() function from the lubridate package provides a way for us to filter by month integer.\n\n# query the data set to only include the months of August, \n# September, and October\nakbs_ds %>% \n  filter(month(date) %in% c(8,9,10)) %>% \n  relocate(deploy_id) %>% \n  collect()\n\n# A tibble: 17,597 × 18\n   deploy_id      ptt instr date                type  quality latitude longitude\n * <chr>        <dbl> <chr> <dttm>              <chr> <chr>      <dbl>     <dbl>\n 1 EB2009_3001… 74626 Mk10  2009-08-01 00:03:13 Fast… 9           70.8     -147.\n 2 EB2009_3001… 74626 Mk10  2009-08-01 00:07:15 Argos B           70.8     -147.\n 3 EB2009_3001… 74626 Mk10  2009-08-01 00:11:39 Argos B           70.8     -147.\n 4 EB2009_3001… 74626 Mk10  2009-08-01 00:17:06 Argos A           70.8     -147.\n 5 EB2009_3001… 74626 Mk10  2009-08-01 00:53:26 Argos B           70.8     -147.\n 6 EB2009_3001… 74626 Mk10  2009-08-01 01:46:52 Argos A           70.8     -147.\n 7 EB2009_3001… 74626 Mk10  2009-08-01 01:55:08 Argos A           70.8     -147.\n 8 EB2009_3001… 74626 Mk10  2009-08-01 02:26:28 Argos B           70.8     -147.\n 9 EB2009_3001… 74626 Mk10  2009-08-01 03:00:20 Argos A           70.8     -147.\n10 EB2009_3001… 74626 Mk10  2009-08-01 03:28:49 Argos B           70.7     -147.\n# … with 17,587 more rows, and 10 more variables: error_radius <dbl>,\n#   error_semi_major_axis <dbl>, error_semi_minor_axis <dbl>,\n#   error_ellipse_orientation <dbl>, offset <lgl>, offset_orientation <lgl>,\n#   gpe_msd <lgl>, gpe_u <lgl>, count <lgl>, comment <chr>\n\n\n\n\nA duckdb example\nDuckDB is a powerful in-process database management system that is easily installed as an R package and provides built-in support for our arrow/parquet data. With the duckdb package as well as DBI, we can pass standard SQL code to query our data set. This is especially useful if you are familiar/comfortable with SQL and you want to develop some fairly complex queries for your data.\n\nlibrary(duckdb)\n# open connection to DuckDB\ncon <- dbConnect(duckdb::duckdb())\n\n# register the data set as a DuckDB table, and give it a name\nduckdb::duckdb_register_arrow(con, \"akbs_locs\", akbs_ds)\n\n# query\nres <- dbGetQuery(con, \n           \"SELECT deploy_id, COUNT(*) AS n_locs \n           FROM akbs_locs \n           GROUP BY deploy_id\")\n\n# clean up\nduckdb_unregister(con, \"akbs_locs\")\ndbDisconnect(con, shutdown = TRUE)\n\ntibble(res)\n\n# A tibble: 6 × 2\n  deploy_id           n_locs\n  <chr>                <dbl>\n1 EB2009_3000_06A1346   6491\n2 EB2011_3000_10A0219   9698\n3 EB2009_3001_06A1332   5660\n4 EB2011_3001_10A0552   7559\n5 EB2009_3002_06A1357   4913\n6 EB2011_3002_10A0200  10197\n\n\nAn example of a more complex SQL query that might be of interest is to identify records with duplicate date-time columns within each deployment. We can do this with DuckDB and SQL.\n\ncon <- dbConnect(duckdb::duckdb())\n\n# register the data set as a DuckDB table, and give it a name\nduckdb::duckdb_register_arrow(con, \"akbs_locs\", akbs_ds)\n\n# query\nres <- dbGetQuery(con, \n           \"SELECT a.deploy_id, a.date, a.quality,\n           a.latitude, a.longitude, a.error_radius\n            FROM akbs_locs a\nJOIN (SELECT deploy_id, date, COUNT(*),\nFROM akbs_locs \nGROUP BY deploy_id, date\nHAVING count(*) > 1 ) b\nON a.deploy_id = b.deploy_id AND\na.date = b.date\nORDER BY a.deploy_id, a.date\")\n\n# clean up\nduckdb_unregister(con, \"akbs_locs\")\ndbDisconnect(con, shutdown = TRUE)\n\ntibble(res)\n\n# A tibble: 4,820 × 6\n   deploy_id         date                quality latitude longitude error_radius\n   <chr>             <dttm>              <chr>      <dbl>     <dbl>        <dbl>\n 1 EB2009_3000_06A1… 2009-06-25 02:21:37 A           66.5     -163.          210\n 2 EB2009_3000_06A1… 2009-06-25 02:21:37 A           66.5     -163.          461\n 3 EB2009_3000_06A1… 2009-06-28 01:10:04 B           66.7     -163.         2154\n 4 EB2009_3000_06A1… 2009-06-28 01:10:04 B           66.8     -163.          661\n 5 EB2009_3000_06A1… 2009-06-30 01:24:03 B           67.6     -164.         5687\n 6 EB2009_3000_06A1… 2009-06-30 01:24:03 B           67.6     -164.          398\n 7 EB2009_3000_06A1… 2009-07-02 09:34:28 B           69.1     -166.         1736\n 8 EB2009_3000_06A1… 2009-07-02 09:34:28 B           69.1     -166.         2475\n 9 EB2009_3000_06A1… 2009-07-02 15:54:47 B           69.2     -166.         1246\n10 EB2009_3000_06A1… 2009-07-02 15:54:47 B           69.2     -166.          522\n# … with 4,810 more rows\n\n\nIf we look at the above table, we can see the duplicate date column values but also notice the coordinate values are different. Instead of just removing one of the duplicates at random, we can use complex SQL to retain the duplicate value with the lower error_radius.\n\ncon <- dbConnect(duckdb::duckdb())\n\n# register the data set as a DuckDB table, and give it a name\nduckdb::duckdb_register_arrow(con, \"akbs_locs\", akbs_ds)\n\n# query\nres <- dbGetQuery(con,\n           \"select a.*\n            from\n            (select *\n            ,ROW_NUMBER() over (\n            partition by deploy_id, date\n            order by error_radius ASC) rn \n            from akbs_locs) a\n            where a.rn = 1\n            order by deploy_id, date, error_radius\")\n\n# clean up\nduckdb_unregister(con, \"akbs_locs\")\ndbDisconnect(con, shutdown = TRUE)\n\ntibble(res)\n\n# A tibble: 42,047 × 19\n     ptt instr date                type  quality latitude longitude error_radius\n   <dbl> <chr> <dttm>              <chr> <chr>      <dbl>     <dbl>        <dbl>\n 1 74627 Mk10  2009-06-23 00:00:00 User  <NA>        66.4     -162.           NA\n 2 74627 Mk10  2009-06-23 02:41:12 Argos A           66.3     -163.          476\n 3 74627 Mk10  2009-06-23 03:07:11 Argos B           66.4     -162.          658\n 4 74627 Mk10  2009-06-23 04:08:10 Argos B           66.3     -162.         5178\n 5 74627 Mk10  2009-06-23 04:15:36 Argos B           66.3     -162.         3877\n 6 74627 Mk10  2009-06-23 04:27:12 Argos A           66.3     -162.          157\n 7 74627 Mk10  2009-06-23 04:44:54 Argos B           66.3     -162.          432\n 8 74627 Mk10  2009-06-23 05:56:58 Argos A           66.3     -163.          174\n 9 74627 Mk10  2009-06-23 06:24:33 Argos A           66.4     -162.          262\n10 74627 Mk10  2009-06-23 07:35:42 Argos B           66.4     -162.          972\n# … with 42,037 more rows, and 11 more variables: error_semi_major_axis <dbl>,\n#   error_semi_minor_axis <dbl>, error_ellipse_orientation <dbl>, offset <lgl>,\n#   offset_orientation <lgl>, gpe_msd <lgl>, gpe_u <lgl>, count <lgl>,\n#   comment <chr>, deploy_id <chr>, rn <dbl>"
  },
  {
    "objectID": "reading-data.html#create-our-location-dataset",
    "href": "reading-data.html#create-our-location-dataset",
    "title": "Reading Source data",
    "section": "Create Our Location Dataset",
    "text": "Create Our Location Dataset\nWe’re going to rely on the typical dplyr approach to query our source data and distill it down to only the essential columns needed for our analysis. Also, since we have a combination of Argos and FastGPS data, we need to create a separate column for the number of satellites used during the fastloc GPS calculation.\n\nakbs_ds <- arrow::open_dataset(dir_out)\n\nakbs_locs <- akbs_ds %>% \n  dplyr::select(deploy_id,\n                date,\n                type,\n                quality,\n                latitude,\n                longitude,\n                error_radius,\n                error_semi_major_axis,\n                error_semi_minor_axis,\n                error_ellipse_orientation) %>% \n  mutate(\n    num_sats = case_when(\n      type == \"FastGPS\" ~ quality\n      ),\n    quality = case_when(\n      type == \"FastGPS\" ~ NA\n    ),\n    num_sats = as.integer(num_sats)\n    ) %>% \n  collect()"
  },
  {
    "objectID": "reading-data.html#remove-duplicate-date-time-records",
    "href": "reading-data.html#remove-duplicate-date-time-records",
    "title": "Reading Source data",
    "section": "Remove Duplicate Date-Time Records",
    "text": "Remove Duplicate Date-Time Records\nIt’s not unusual for a small number of records within a deployment to have duplicate times. We will want to identify and remove thiese records early in the process.\n\nakbs_locs <- akbs_locs %>% \n  group_by(deploy_id) %>% \n  arrange(date, error_radius) %>% \n  mutate(\n    rank = 1L,\n    rank = case_when(duplicated(date, fromLast = FALSE) ~\n                              lag(rank) + 1L, TRUE ~ rank)) %>% \n  dplyr::filter(rank == 1)"
  },
  {
    "objectID": "reading-data.html#convert-to-spatial-feature",
    "href": "reading-data.html#convert-to-spatial-feature",
    "title": "Reading Source data",
    "section": "Convert to Spatial Feature",
    "text": "Convert to Spatial Feature\nSince our data are, at the core, spatial observations we will benefit by converting our akbs_locs tibble/data.frame into a spatial point feature with the sfpackage. In order to accomplish this, we need to identify our coordinate columns and know the coordinate reference system (CRS). For all Argos and GPS data, the coordinate reference system is geographic with X and Y represented as longitude (-180, 180) and latitude (-90, 90). You should ensure the data are formatted as decimal degrees and not some combination of degrees, minutes, and seconds. To specify the CRS, we’ll rely on the designated EPSG code of 4326 which tells sf that our data are geographic.\n\nlibrary(sf)\n\nakbs_locs <- sf::st_as_sf(akbs_locs, coords = c(\"longitude\",\"latitude\"),\n                          crs = \"epsg:4326\")\n\nakbs_locs %>% \n  dplyr::select(deploy_id, date, geometry)\n\nSimple feature collection with 42047 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -175.137 ymin: -15.7054 xmax: 170.878 ymax: 73.478\nGeodetic CRS:  WGS 84\n# A tibble: 42,047 × 3\n# Groups:   deploy_id [6]\n   deploy_id           date                           geometry\n   <chr>               <dttm>                      <POINT [°]>\n 1 EB2009_3000_06A1346 2009-06-23 00:00:00     (-162.42 66.38)\n 2 EB2009_3000_06A1346 2009-06-23 02:41:12 (-162.5562 66.3168)\n 3 EB2009_3000_06A1346 2009-06-23 03:07:11 (-162.4431 66.3656)\n 4 EB2009_3000_06A1346 2009-06-23 04:08:10 (-162.3752 66.3216)\n 5 EB2009_3000_06A1346 2009-06-23 04:15:36 (-162.3757 66.3222)\n 6 EB2009_3000_06A1346 2009-06-23 04:27:12  (-162.3922 66.345)\n 7 EB2009_3000_06A1346 2009-06-23 04:44:54 (-162.4411 66.3316)\n 8 EB2009_3000_06A1346 2009-06-23 05:56:58 (-162.5032 66.3487)\n 9 EB2009_3000_06A1346 2009-06-23 06:24:33 (-162.4993 66.3518)\n10 EB2009_3000_06A1346 2009-06-23 07:35:42 (-162.4958 66.3583)\n# … with 42,037 more rows\n\n\nWe can see that instead of separate columns for longitude and latitude we now have a single geometry column that describes the point geometry of each location. Also note that the metadata indicates the Geodetic CRS is specified as WGS 84 which tells is our CRS specification is correctly set to geographic with longitude/latitude coordinate values."
  },
  {
    "objectID": "crawl-utils.html",
    "href": "crawl-utils.html",
    "title": "Easier crawl-ing with crawlUtils",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "about-authors.html",
    "href": "about-authors.html",
    "title": "About the Authors",
    "section": "",
    "text": "Drs. Josh M. London and Devin S. Johnson are researchers with the NOAA National Marine Fisheries Service. Dr. London is a wildlife biologist at the Alaska Fisheries Science Center’s Marine Mammal Laboratory in Seattle, Washington. Dr. Johnson is a mathematical statistician at the Pacific Islands Fisheries Science Center’s Protected Species Division in Honolulu, Hawaii. Dr. London has over 15 years of experience programming and deploying satellite tags on phocid seals. He has also developed various workflows for the management of telemetry data in R. Dr. Johnson is a leading statistical ecologist with expertise in the analysis of animal movement. Dr. Johnson is the lead author and developer of the R package crawl. Dr. London is the lead author of the R package pathroutr and co-developer of crawl."
  },
  {
    "objectID": "behavior-plots.html",
    "href": "behavior-plots.html",
    "title": "Exploring Animal Behavior",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "pathroutr-demo.html",
    "href": "pathroutr-demo.html",
    "title": "A pathroutr Harbor Seal Example",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "intro-to-pathroutr.html",
    "href": "intro-to-pathroutr.html",
    "title": "Introduction to the pathroutr Package",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "Package Installation & Dependencies",
    "section": "",
    "text": "The examples and workflows described here are all developed with R version 4.2 and you are strongly encouraged to update your version to 4.2 or greater. In general, the code and examples provided should work reliably with any version of R greater than 4.0."
  },
  {
    "objectID": "packages.html#install-core-packages",
    "href": "packages.html#install-core-packages",
    "title": "Package Installation & Dependencies",
    "section": "Install Core Packages",
    "text": "Install Core Packages\nThe focus of this site, and the example workflows we describe, is on animal movement modeling in R using the {crawl} package. In addition, the recently developed {pathroutr} and {crawlUtils} packages are key enhancements that extend the capabilities of {crawl} and improve the user experience.\nYou are strongly encouraged to install the latest available versions of each package.\n\nInstall {crawl}\n\nInstall via CRAN\n{crawl} is currently available on CRAN and R >= 4.0 is highly recommended.\n\n# install latest version of crawl from CRAN\ninstall.packages(\"crawl\")\n\n\n\nInstall via R-Universe\nThe latest version of {crawl} is also available via R-Universe.\n\n# Install crawl from my R-Universe repository\n# Enable repository from dsjohnson\noptions(repos = c(\n  dsjohnson = 'https://dsjohnson.r-universe.dev',\n  CRAN = 'https://cloud.r-project.org'))\n# Download and install crawl in R\ninstall.packages('crawl')\n# Browse the crawl manual pages\nhelp(package = 'crawl')\n\nYou can also add the repository to your local list of repositories in your .Rprofile and this will ensure update.packages() pulls any new releases of {crawl} from R-Universe\n\n#install.packages(\"usethis\")\nusethis::edit_r_profile()\n# add the following text or replace existing repos option\noptions(repos = c(dsjohnson = 'https://dsjohnson.r-universe.dev',\n                  CRAN = 'https://cloud.r-project.org'))\n\n\n\nInstall via Github\nA development version of {pathroutr} is also available from GitHub. This version should be used with caution and only after consulting with package authors.\n\n# install.packages(\"remotes\")\nremotes::install_github(\"NMML/crawl@devel\")\n\n\n\n\nInstall {pathroutr}\n{pathroutr} is currently not available on CRAN and also requires R >= 4.0. Please upgrade your version of R, if needed, before proceeding. Future versions of {pathroutr} may support pre-4.0 versions of R. But, for now, only 4.0+ is supported.\n\nInstall via R-Universe\nStarting with v0.2.1, {pathroutr} is available via R-Universe.\n\n# Install new pathroutr version from my R-Universe repository\ninstall.packages(\"pathroutr\", repos = \"https://jmlondon.r-universe.dev\")\n\nYou can also add my repository to your local list of repositories in your .Rprofile and this will ensure update.packages() pulls any new releases of {pathroutr}\n\n#install.packages(\"usethis\")\nusethis::edit_r_profile()\n# add the following text or replace existing repos option\noptions(repos = c(jmlondon = 'https://jmlondon.r-universe.dev',\n                  CRAN = 'https://cloud.r-project.org'))\n\n\n\nInstall via Github\nThe development version of {pathroutr} is available from GitHub with:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"jmlondon/pathroutr\")\n\n\n\n\nInstall {crawlUtils}\n\nInstall via R-Universe\n\n# Install crawlUtils from Devin's R-Universe repository\noptions(repos = c(\n  dsjohnson = 'https://dsjohnson.r-universe.dev',\n  CRAN = 'https://cloud.r-project.org'))\n# Download and install crawlUtils in R\ninstall.packages('crawlUtils')\n# Browse the crawlUtils manual pages\nhelp(package = 'crawlUtils')\n\nYou can also add the repository to your local list of repositories in your .Rprofile and this will ensure update.packages() pulls any new releases of {crawlUtils} from R-Universe\n\n#install.packages(\"usethis\")\nusethis::edit_r_profile()\n# add the following text or replace existing repos option\noptions(repos = c(dsjohnson = 'https://dsjohnson.r-universe.dev',\n                  CRAN = 'https://cloud.r-project.org'))\n\n\n\nInstall via Github\nThe development version of {crawlUtils} is available from GitHub with:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"dsjohnson/crawlUtils\")"
  },
  {
    "objectID": "history-of-crawl.html",
    "href": "history-of-crawl.html",
    "title": "History of crawl and Movement Models in R",
    "section": "",
    "text": "This is a Quarto website"
  }
]