[
  {
    "objectID": "data-sources.html",
    "href": "data-sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Where and how you access the source data for your telemetry study will depend on the type of tag and vendor. Argos location data is available to users from the Argos website and many third party sites and repositories (e.g. movebank, sea-turtle.org) have API connections to ArgosWeb that can facilitate data access. Each tag manufacturer often has additional data streams included within the Argos transmission that require specific software or processing to translate. Both the Sea Mammal Research Unit (SMRU) and Wildlife Computers provide online data portals that provide users access to the location and additional sensor data.\nRegardless of how the data are retrieved, these files should be treated as read only and not edited. ArgosWeb and vendors have, typically, kept the data formats and structure consistent over time. This affords end users the ability to develop custom processing scripts without much fear the formats will change and break their scripts."
  },
  {
    "objectID": "data-sources.html#data-management",
    "href": "data-sources.html#data-management",
    "title": "Data Sources",
    "section": "Data Management",
    "text": "Data Management\nThe quantity and diversity of data generated from bio-logging studies can quickly become overwhelming especially in situations where a single device is deployed for several months and collects data from multiple on-board sensors. For some research projects, storing these data in a central relational database (e.g. PostgreSQL) can have important benefits. If storing bio-logger data in a relational database, explore one that provides native support or extensions for spatial data types and operations (e.g. PostGIS, SpatiaLite). For many, though, simply storing data as and organized collection of plain text files (e.g. *.csv) is sufficient. This approach can be further improved by converting the comma-separated files to parquet files and leaning on the the Apache arrow package for R. These parquet files provide modern and efficient file compression, a columnar-based structure, and are optimized for big data. Even if a project starts as small data, it’s often useful to design your data management with growth in mind."
  },
  {
    "objectID": "data-sources.html#wildlife-computers-data-sources",
    "href": "data-sources.html#wildlife-computers-data-sources",
    "title": "Data Sources",
    "section": "Wildlife Computers Data Sources",
    "text": "Wildlife Computers Data Sources\nWe have the most first-hand experience with data originating from Wildlife Computers devices and data that have been processed through the Wildlife Computers Data Portal. This doesn’t mean the techniques, packages, and analysis presented require Wildlife Computers devices. Or, that most of what is presented is not applicable to other bio-logging data sources. When possible, we’ve tried to create examples and processes that are agnostic to the type of deployed device.\n\n\n\n\n\n\nThe wcUtils Package\n\n\n\nFrequent users of the Wildlife Computers Data Portal or data from their bio-loggers processed through the DAP program may find the wcutils package for R useful. This package is maintained by Josh London and provides several utility function for downloading data from the WCDP and for tidy processing of typical data files."
  },
  {
    "objectID": "path-routing-history.html",
    "href": "path-routing-history.html",
    "title": "Routing Marine Animal Paths Around Land",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "pathroutr-demo2.html",
    "href": "pathroutr-demo2.html",
    "title": "A pathroutr Bearded Seal Example",
    "section": "",
    "text": "We’re going to revisit our example of bearded seal movement along the northwest Alaska coast. And, we’re going to walk through this demo relying on the helper functions included within the crawlUtils package.\n\nlibrary(crawl)\n\ncrawl 2.3.0 (2022-05-13) \n Demos and documentation can be found at our new GitHub repository:\n https://dsjohnson.github.io/crawl_examples/\n\nlibrary(crawlUtils)\n\ncrawlUtils 0.1.02 (2022-06-23)\n\nlibrary(pathroutr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:crawl':\n\n    flatten\n\nlibrary(ggplot2)\nlibrary(colorspace)\nlibrary(sf)\n\nLinking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE"
  },
  {
    "objectID": "pathroutr-demo2.html#sourcing-landbarrier-data",
    "href": "pathroutr-demo2.html#sourcing-landbarrier-data",
    "title": "A pathroutr Bearded Seal Example",
    "section": "Sourcing Land/Barrier Data",
    "text": "Sourcing Land/Barrier Data\nThe first thing we need to do is source the relevant land (or other barrier) polygon data for our study area. The crawlUtils package has a built-in function for downloading a global coastline data file based on the OpenStreetMap data. This is a relatively large file so the cu_download_osm() function downloads a local copy for you. The initial download is likely between 750 MG and 1 GB of data.\n\ncrawlUtils::cu_download_osm()\n# > This function will download a considerable amount of coastline data.\n# > Are you sure you want to proceed? [y/n]: y\n\nWe, obviously, don’t need the entire global coastline for our study. So, we will want to crop the downloaded data to our study area. But, it’s important that we provide a sensible buffer to fully capture the available land. There’s no exact science for this value, but can be especially important in smaller areas with complicated coastline. FOr this example, we’ll set the buffer to 100km.\n\nsim_bb <- map(akbs_locs$sims, crawlUtils::st_bbox_list, as_sfc=TRUE)\nbb <- crawlUtils::st_bbox_list(sim_bb, as_sfc=TRUE) %>%\n  st_buffer(100000)\n\nThe crawlUtils::cu_osm_coast() function will take the bounding box and apply that to the previously downloaded coastline data from OpenStreetMap.\n\n# Crop OSM coastline previously downloaded\nland <- cu_osm_coast(bb)\n\nLet’s revisit our previous plot and use the downloaded coastline data instead of the data from rnaturalearth package\n\nworld <- land %>% \n  sf::st_transform(st_crs(akbs_preds))\n\nmap <- ggplot() + geom_sf(data = world) +\n  geom_sf(data = akbs_sims, aes(color = deploy_id), alpha = 0.1, size=0.5) +\n  geom_sf(data = akbs_preds, aes(color = deploy_id),\n          size = 0.3) + \n  coord_sf(\n    xlim = sf::st_bbox(akbs_sims)[c(1,3)],\n    ylim = sf::st_bbox(akbs_sims)[c(2,4)]) +\n  scale_color_discrete_qualitative(\n    palette = \"Dark 3\",\n    name = \"deployment\") +\n  labs(title = \"Predicted Movement of 6 Bearded Seals in NW Alaska\",\n        subtitle = paste0(\"most likely path and imputed paths shown\"),\n       caption = \"coastline data provided by OpenStreetMap\") +\n  theme_minimal()\n\nmap"
  },
  {
    "objectID": "outlier-locs.html",
    "href": "outlier-locs.html",
    "title": "Removing Outlier Locations",
    "section": "",
    "text": "Location estimates from bio-loggers can sometimes provide extremely erroneous locations. This is especially true for the Argos system and the error estimates provided do not provide a realistic mechanism for removal (e.g. remove all locations with an error estimate above some threshold). If these extreme outliers are not removed from the data set prior to model fitting, issues with model convergence or other errors can arise."
  },
  {
    "objectID": "outlier-locs.html#speed-distance-and-angle-filtering",
    "href": "outlier-locs.html#speed-distance-and-angle-filtering",
    "title": "Removing Outlier Locations",
    "section": "Speed, Distance, and Angle Filtering",
    "text": "Speed, Distance, and Angle Filtering\nTo identify and remove these obviously erroneous observations we will rely on a speed, distance, and angle filter from the trip package to identify locations that would require traveling speeds that exceed 2x or more what would be expected from the study species. In our case, with bearded seals, we’ll use 7.5m/second (the likely maximum sustained speed for a bearded seal is 2.5m/second).\n\nlibrary(dplyr)\nlibrary(sf)\nlibrary(trip)\n\ndat <- akbs_locs %>% \n  ungroup() %>% \n  arrange(deploy_id, date)\n\ndat_tr <- trip(dat, c(\"date\",\"deploy_id\"), correct_all = FALSE)\n\nkeep <- sda(\n          dat_tr,\n          smax = 27 #km/hour or 7.5m/sec *3.6\n        )\n\nakbs_locs <- dat %>% \n  mutate(sda_keep = keep) %>% \n  filter(sda_keep) %>% \n  dplyr::select(-sda_keep)"
  },
  {
    "objectID": "why-tidy.html",
    "href": "why-tidy.html",
    "title": "Why Tidy Telemetry Data",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "troubleshoot.html",
    "href": "troubleshoot.html",
    "title": "Troubleshooting Common Errors",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "map-observations.html",
    "href": "map-observations.html",
    "title": "Mapping Telemetry Observations",
    "section": "",
    "text": "There are a number of workflows within R for making maps based on geospatial data. Here, we focus almost exclusively on mapping within the ggplot2 package ecosystem. But, it is certainly worth the effort to learn about the other options including tmap, mapview, and leaflet."
  },
  {
    "objectID": "map-observations.html#choosing-a-projection",
    "href": "map-observations.html#choosing-a-projection",
    "title": "Mapping Telemetry Observations",
    "section": "Choosing a Projection",
    "text": "Choosing a Projection\nIt’s important that your spatial coordinates be projected. This is an important step when presenting your data as a map because the choice of projection can lead to unintentional perception bias and cause confusion. Just as importantly, geographic data that is represented in latitude and longitude coordinates cannot be used for animal movement modeling. Ideally, you will already have a good understanding of your study area and will have determined your target CRS. In this situation, you can use the sf::st_transform() function to transform your coordinates into this new projection.\nIf you are unsure of what CRS might be a good choice for your data, the crsuggest package can provide some insight. We’ll use this package to find a sensible projection for our Alaska bearded seal data. To help narrow the options, we can specify that we want to keep the corresponding geographic coordinate system as WGS 84 by passing the 4326 EPSG code to the gcs argument. We can also specify that we want our units to be in meters.\n\nlibrary(sf)\nlibrary(crsuggest)\n\ntop_crs <- suggest_crs(akbs_locs,\n            gcs = 4326,\n            units = \"m\")\n\ntop_crs %>% \n  dplyr::select(crs_code, crs_name, crs_gcs, crs_units)\n\n# A tibble: 10 × 4\n   crs_code crs_name                              crs_gcs crs_units\n   <chr>    <chr>                                   <dbl> <chr>    \n 1 6124     WGS 84 / EPSG Arctic zone 4-12           4326 m        \n 2 32602    WGS 84 / UTM zone 2N                     4326 m        \n 3 32601    WGS 84 / UTM zone 1N                     4326 m        \n 4 32603    WGS 84 / UTM zone 3N                     4326 m        \n 5 32604    WGS 84 / UTM zone 4N                     4326 m        \n 6 32605    WGS 84 / UTM zone 5N                     4326 m        \n 7 32606    WGS 84 / UTM zone 6N                     4326 m        \n 8 32607    WGS 84 / UTM zone 7N                     4326 m        \n 9 5926     WGS 84 / EPSG Arctic Regional zone B1    4326 m        \n10 5931     WGS 84 / EPSG Arctic Regional zone C1    4326 m        \n\n\nThe first CRS suggested is the WGS 84 / EPSG Arctic zone 4-12. So, let’s go ahead and transform our data and see what this projection looks like on a map with our data. As mentioned above, we’ll use the sf::st_transform() function to do the transformation. To give ourselves some context, we’ll load simple world basemap data from the rnaturalearth package.\n\n#|: message: false\n#|: warning: false\n#| fig-asp: 1\n#| fig-cap: Basic map of locations from bio-loggers deployed on six\n#|   bearded seals in northwest Alaska\nlibrary(ggplot2)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\nworld <- ne_countries(scale = \"medium\",returnclass = \"sf\") %>% \n  sf::st_make_valid()\ncrs_code <- as.integer(top_crs$crs_code[1])\n \nfilt_locs <- akbs_locs %>% sf::st_transform(crs_code)\nworld <- world %>% sf::st_transform(crs_code) \n\nmap <- ggplot() + geom_sf(data = world) +\n  geom_sf(data = filt_locs, size = 0.5, shape = 19) + \n  coord_sf(\n    xlim = sf::st_bbox(filt_locs)[c(1,3)],\n    ylim = sf::st_bbox(filt_locs)[c(2,4)]) +\n  theme_minimal()\n\nmap\n\n\n\n\nThis looks pretty good and, now, we can proceed with making a more complete set of maps for our observations. The focus is still more about understanding the nature and distribution of our data than making perfect maps. But, it is reasonable to expect these products might be useful within future publications or presentations."
  },
  {
    "objectID": "map-observations.html#map-observations-with-ggplot2",
    "href": "map-observations.html#map-observations-with-ggplot2",
    "title": "Mapping Telemetry Observations",
    "section": "Map Observations with ggplot2",
    "text": "Map Observations with ggplot2\nThere are three methods for visualizing our observation data:\n\npoints on a map, colored by deploy_id\nlines on a map, colored by deploy_id\ngrid of point density\n\n\nPoints on a Map\nFor this, we can start with the figure we made above. But, we’ll want to be more deliberate about grouping, color choices, and labels. For color choices, we’re going to choose palettes from the colorspace package. Also, we’ll take the opportunity to reduce the length of our deploy_id values.\n\nlibrary(colorspace)\nlibrary(stringr)\n\nfilt_locs <- filt_locs %>% \n  dplyr::mutate(deploy_id = stringr::str_sub(deploy_id, 1,11))\n\nmap <- ggplot() + geom_sf(data = world) +\n  geom_sf(data = filt_locs, aes(color = deploy_id),\n          size = 0.35, shape = 19) + \n  coord_sf(\n    xlim = sf::st_bbox(filt_locs)[c(1,3)],\n    ylim = sf::st_bbox(filt_locs)[c(2,4)]) +\n  scale_color_discrete_qualitative(\n    palette = \"Dark 3\",\n    name = \"deployment\") +\n  labs(title = \"Observed Locations of 6 Bearded Seals in NW Alaska\",\n        subtitle = paste0(\"some data were censored based on a\",\n                          \"speed, distance, and angle filter\")) +\n  theme_minimal()\n\nmap\n\n\n\n\nMap of locations from bio-loggers deployed on six bearded seals in northwest Alaska\n\n\n\n\nThis is pretty good, but it would be nice if we could interact with the data and zoom in on particular regions. The mapview package provides an easy means for creating interactive maps of your spatial data. The one caveat to mapview is that it is based on the Web Mercator projection (a la Google Maps, etc) and there are some extra steps required if your tracks cross the dateline at 180 (which ours do … yay! fun!)\nWhat we need to do is transform our data back to geographic and, then, convert the coordinates from -180:180 to 0:360. We will use a custom written function to handle this for us. The st_to_360() function is not needed for data sets that do not cross the 180 meridian.\n\nst_to_360 <- function(g) {\n  coords <- (sf::st_geometry(g) + c(360,90)) %% c(360) - c(0,90)\n  g <- sf::st_set_geometry(g,coords) %>% sf::st_set_crs(4326)\n  return(g)\n}\n\nWe will use the ESRI Ocean Basemap for our background layer and the same qualitative color palette we used above. The mapview package is based on the web mapping library, leaflet. Because of that, these interactive maps are only available when rendering to HTML or other HTML capable viewers.\n\nlibrary(mapview)\n\nsf::st_transform(filt_locs,4326) %>% st_to_360() %>% \nmapview::mapview(map.types = \"Esri.OceanBasemap\",\n                 zcol = \"deploy_id\",\n                 layer.name = \"Deployment ID\",\n                 col.regions = qualitative_hcl(palette = \"Dark 3\", n =6),\n                 cex = 1.5, lwd = 0)\n\n\n\n\nInteractive map of locations from bio-loggers deployed on six bearded seals in northwest Alaska\n\n\n\n\nLines on a Map\nIn actuality, the point observations from our deployments represent an underlying movement path for each animal. And, later, we’ll work to create a movement model that estimates this path. But, for now, we can approximate this by simply connecting the points into an ordered path. The sf package can represent a range of spatial data types, including LINESTRING. Since we have multiple lines within our data set, we’ll create a MULTILINESTRING. Creating a spatial line from a set of spatial points isn’t, initially, intuitive. We need to ensure key attributes are retained during the conversion. First, we need to identify our ‘grouping’ variable. In this case, as with our points, the deploy_id identifies our groups. Within each group, our points need to be properly ordered. The date column provides this order for us. Lastly, when we combine the individual points into a line, all of the point level attributes (e.g. date, location type, error) will be lost. If there are key attributes that should be retained, then those need to either be included as grouping variables or stored in a table that can be, later, joined with the lines data. We are going to keep things simple and not worry about line attributes beyond deploy_id.\n\nfilt_lines <- filt_locs %>% \n  dplyr::group_by(deploy_id) %>% \n  dplyr::arrange(date) %>% \n  dplyr::summarise(do_union = FALSE) %>% \n  sf::st_cast(\"MULTILINESTRING\")\n\nWe can use much of the same code as before and just replace filt_locs with filt_lines.\n\nlibrary(colorspace)\nlibrary(stringr)\n\nmap <- ggplot() + geom_sf(data = world) +\n  geom_sf(data = filt_lines, aes(color = deploy_id),\n          lwd = 0.5) + \n  coord_sf(\n    xlim = sf::st_bbox(filt_lines)[c(1,3)],\n    ylim = sf::st_bbox(filt_lines)[c(2,4)]) +\n  scale_color_discrete_qualitative(\n    palette = \"Dark 3\",\n    name = \"deployment\") +\n  labs(title = \"Connected Movement Paths of 6 Bearded Seals in NW Alaska\",\n        subtitle = paste0(\"some data were censored based on a\",\n                          \"speed, distance, and angle filter\")) +\n  theme_minimal()\n\nmap\n\n\n\n\nMap of connected movement paths from bio-loggers deployed on six bearded seals in northwest Alaska\n\n\n\n\nAnd, just as before, we can create an interactive map with mapview, this time, showing our connected movement paths. Note that, before, we relied on the col.regions parameter to specify colors for our points. For lines, we specify the color palette with color. Also, note, the lwd was increased to 1.5 and the cex specification was removed.\n\nlibrary(mapview)\n\nsf::st_transform(filt_lines,4326) %>% st_to_360() %>% \nmapview::mapview(map.types = \"Esri.OceanBasemap\",\n                 zcol = \"deploy_id\",\n                 layer.name = \"Deployment ID\",\n                 color = qualitative_hcl(palette = \"Dark 3\", n =6),\n                 lwd = 1.5)\n\n\n\n\nInteractive map of connected movement paths from bio-loggers deployed on six bearded seals in northwest Alaska\n\n\n\n\nPoint Density Map\nOver-plotting occurs when multiple points are plotted on top of one another because of their close proximity. This can make it difficult fully understand the distribution of points. To address this, we can create a uniformspatial grid and summarize the number of points within each grid cell. This density grid should provide a more representative visualization of our data.\nThe first step in this process is to create a spatial grid tesselation. We need to decide on a cell resolution and the shape of our grid cells. There are no specific rules regarding grid cell resolution. Although, the memory and computational time required to process grid data increases quickly as changes in cell dimensions lead to non-linear increases in the number of cells. A spatial grid can be any regular tesselation of the study area. We have three possible regular tesselations: equilateral triangles, squares, and regular hexagons. Of those, squares are the most common but hexagons have some key benefits.\n\n\n\n\n\n\nWhy Hexagons\n\n\n\nThe following explanation comes from Matt Strimas-Mackey at https://strimas.com/post/hexagonal-grids/\nRegular hexagons are the closest shape to a circle that can be used for the regular tessellation of a plane and they have additional symmetries compared to squares. These properties give rise to the following benefits.\n\nReduced edge effects: a hexagonal grid gives the lowest perimeter to area ratio of any regular tessellation of the plane. In practice, this means that edge effects are minimized when working with hexagonal grids. This is essentially the same reason beehives are built from hexagonal honeycomb: it is the arrangement that minimizes the amount of material used to create a lattice of cells with a given volume.\nAll neighbours are identical: square grids have two classes of neighbours, those in the cardinal directions that share an edge and those in diagonal directions that share a vertex. In contrast, a hexagonal grid cell has six identical neighbouring cells, each sharing one of the six equal length sides. Furthermore, the distance between centroids is the same for all neighbours.\nBetter fit to curved surfaces: when dealing with large areas, where the curvature of the earth becomes important, hexagons are better able to fit this curvature than squares. This is why soccer balls are constructed of hexagonal panels.\n\n\n\nCreating our grid within R is pretty straightforward using the sf::st_make_grid() function. We’ll set our cellsize to 50,000 meters (25 km) and, then, create an index for each of our cells. The sf::st_join() function will intersect the hexagonal grid with our locations so we can count the number of locations within each cell. One thing we need to decide is whether to aggregated the counts across all observations or keep the grouped approach and determine separate counts for each deploy_id. Since we are still interested in understanding the distribution and nature of our data as best as possible, it seems sensible to keep the count separated by deploy_id. However, with more than 6 deployments, this could become untenable to plot and fully explore.\n\nhexgrid <- sf::st_make_grid(st_bbox(filt_locs) %>% st_as_sfc(), \n                            cellsize = 50*1000,\n                            what = \"polygons\", square = FALSE) \nhexgrid <- sf::st_sf(index = 1:length(lengths(hexgrid)), hexgrid)\n\nhexbin <- sf::st_join(filt_locs, hexgrid, join = st_intersects)\n\nlocs_count <- hexgrid %>%\n  dplyr::left_join(\n    dplyr::count(hexbin, index, deploy_id) %>%\n      tibble::as_tibble() %>%\n      dplyr::select(deploy_id, index, ct=n)\n  ) %>% tidyr::drop_na()\n\n\nlibrary(scico)\nmap <- ggplot() + geom_sf(data = world) +\n  geom_sf(data = locs_count, \n          size = 0.125,\n          aes(fill = ct)) +\n  coord_sf(\n    xlim = sf::st_bbox(filt_lines)[c(1,3)],\n    ylim = sf::st_bbox(filt_lines)[c(2,4)]) +\n  scale_fill_scico(palette = 'imola',\n                    trans = \"log10\",\n    name = \"number of locations\",\n    guide = guide_colorbar(title.position = \"bottom\", barwidth = 15, \n                           barheight = 0.5, title.hjust = 0.5)) + \n  facet_wrap(~ deploy_id) +\n  theme(legend.position = \"bottom\") +\n  ggtitle(\"Spatial distribution of bearded seal bio-logger locations\") +\n  labs(caption = bquote(\"One hexagonal cell represents 50\"~km^2))\n  \nmap\n\n\n\n\nMaps showing the spatial distribution of location observations from bio-loggers deployed on six bearded seals in northwest Alaska"
  },
  {
    "objectID": "summary-tables.html",
    "href": "summary-tables.html",
    "title": "Deployment Summary Tables",
    "section": "",
    "text": "Now that we’ve setup a data structure for efficient access, imported the source data into R, and converted that data into a spatial data set, it’s time to explore and see what we have to work with. This is an important step so you can recognize any problems with the data or inconsistencies that need to be further investigated.\nSummary tables are a good way of splitting large data into components of interest and learning how our data might be distributed across those components. One example might be a simple calculation of the number of location observations within each month across the individual animals. This might identify anomalies such as locations in months prior to deployment or missing data when it was expected.\nWe’ll first want to group our location records by deployment and month.\n\nlibrary(sf)\nlibrary(lubridate)\nlibrary(dplyr)\n\ndat <- akbs_locs %>% \n  sf::st_drop_geometry() %>% \n  mutate(month = lubridate::month(date)) %>% \n  group_by(deploy_id, month) %>% \n  count(name = \"num_locs\") \n\nTo create a sensible table, let’s just focus on a single deploy_id, EB2009_3000_06A1346\n\nlibrary(knitr)\n\ndat %>% \n  dplyr::filter(deploy_id == \"EB2009_3000_06A1346\") %>% \n  dplyr::arrange(month) %>% \n  knitr::kable()\n\n\n\n\ndeploy_id\nmonth\nnum_locs\n\n\n\n\nEB2009_3000_06A1346\n1\n307\n\n\nEB2009_3000_06A1346\n2\n446\n\n\nEB2009_3000_06A1346\n3\n567\n\n\nEB2009_3000_06A1346\n4\n153\n\n\nEB2009_3000_06A1346\n6\n183\n\n\nEB2009_3000_06A1346\n7\n778\n\n\nEB2009_3000_06A1346\n8\n1007\n\n\nEB2009_3000_06A1346\n9\n764\n\n\nEB2009_3000_06A1346\n10\n748\n\n\nEB2009_3000_06A1346\n11\n753\n\n\nEB2009_3000_06A1346\n12\n476\n\n\n\n\n\nOne oddity that immediately stands out is the lack of locations in May. This, however, is to be expected as these deployments started in June and stopped transmitting in April of the following year and matches expectations for battery life.\nIf we look at another deployment, EB2009_3001_06A1332, we can see that this deployment ended in March.\n\ndat %>% \n  dplyr::filter(deploy_id == \"EB2009_3001_06A1332\") %>% \n  dplyr::arrange(month) %>% \n  knitr::kable()\n\n\n\n\ndeploy_id\nmonth\nnum_locs\n\n\n\n\nEB2009_3001_06A1332\n1\n476\n\n\nEB2009_3001_06A1332\n2\n458\n\n\nEB2009_3001_06A1332\n3\n63\n\n\nEB2009_3001_06A1332\n6\n119\n\n\nEB2009_3001_06A1332\n7\n735\n\n\nEB2009_3001_06A1332\n8\n641\n\n\nEB2009_3001_06A1332\n9\n792\n\n\nEB2009_3001_06A1332\n10\n789\n\n\nEB2009_3001_06A1332\n11\n581\n\n\nEB2009_3001_06A1332\n12\n677\n\n\n\n\n\nLastly, deployment EB2011_3001_10A0552 appears to have stopped transmitting in January of the following year which is 3-4 months earlier than any of the other devices.\n\ndat %>% \n  dplyr::filter(deploy_id == \"EB2011_3001_10A0552\") %>% \n  dplyr::arrange(month) %>% \n  knitr::kable()\n\n\n\n\ndeploy_id\nmonth\nnum_locs\n\n\n\n\nEB2011_3001_10A0552\n1\n675\n\n\nEB2011_3001_10A0552\n6\n482\n\n\nEB2011_3001_10A0552\n7\n993\n\n\nEB2011_3001_10A0552\n8\n1062\n\n\nEB2011_3001_10A0552\n9\n1134\n\n\nEB2011_3001_10A0552\n10\n904\n\n\nEB2011_3001_10A0552\n11\n896\n\n\nEB2011_3001_10A0552\n12\n1019\n\n\n\n\n\nThis is not so unusual, but such an anomaly is worth investigating further to ensure there were no issues with the data processing or other important deployment metadata."
  },
  {
    "objectID": "key-concepts.html",
    "href": "key-concepts.html",
    "title": "Introduction to Key Concepts",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "input-data.html",
    "href": "input-data.html",
    "title": "Preparing Model Inputs",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "pathroutr-cautions.html",
    "href": "pathroutr-cautions.html",
    "title": "Cautions When Using pathroutr",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "tidy-messy-data.html",
    "href": "tidy-messy-data.html",
    "title": "Tidy-ing Messy Data for Easy Modeling",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Tidy Movement Modeling with R",
    "section": "",
    "text": "Over the years, we have produced a number of online resources intended to provide insights and examples for modeling animal movement with the crawl and pathroutr packages. Some of these documents and examples have aged well; others have not. R, the tidyverse, animal movement theory, and our experience evolve quickly. What we might have proposed as the best approach 5 years ago (or, sometimes even 5 months ago) may not be the case today.\nThis website is being developed to provide a more authoritative resource that we hope to maintain more reliably than previous attempts. The bio-logging and animal movement modeling community can also play a role by helping us improve the content. Identification of errors and suggested improvements are strongly encouraged. If you find an error or have ideas for improvement, please open an issue via the GitHub repository. We will also accept pull requests for any bug fixes or enhancements."
  },
  {
    "objectID": "index.html#a-pragmatic-guide",
    "href": "index.html#a-pragmatic-guide",
    "title": "Welcome to Tidy Movement Modeling with R",
    "section": "A Pragmatic Guide",
    "text": "A Pragmatic Guide\nThe crawl package for R (and supporting friends e.g. pathroutr, momentuHMM) is designed and built with the idea that it should be accessible and useful to a research biologist with some intermediate R skills and an understanding of the basic statistical theory behind the analysis of animal movement. This website will focus on providing the user with suggested principles, workflows, and pragmatic approaches that, if followed, should make analysis with crawl more efficient and reliable.\n\n\n\n\n\n\nTelemetry data are collected on a wide range of species and come in a number of formats and data structures. The code and examples provided here were developed from data the authors are most familiar with. You will very likely NOT be able to just copy and paste the code and apply to your data. We suggest you focus more on understanding what the code is doing and then write your own version of the code to meet your data and your research needs."
  },
  {
    "objectID": "index.html#content-outline",
    "href": "index.html#content-outline",
    "title": "Welcome to Tidy Movement Modeling with R",
    "section": "Content Outline",
    "text": "Content Outline\nInformation on this site is organized into the following sections:\n\nWelcome\n\nintroduction to the website (this page)\nabout the authors\npackage dependencies\n\nTelemetry Data is Messy\n\ndata sources\nwhy tidy telemetry data\ntidying messy data for easy modeling\n\nData Exploration\n\ndeployment summary tables\nmapping telemetry observations\nexploring animal behavior\n\nMovement Modeling with crawl\n\nintroduction to key concepts\nhistory of crawl and mov’t models in R\npreparing model inputs\neasier crawl-ing with crawlUtils\ntroubleshooting common errors\n\nOh No! Paths Cross Land!\n\nrouting marine animal paths around land\nintroduction to the pathroutr package\nsourcing a land/barrier polygon\npathroutr harbor seal example\ncautions when using pathroutr\n\n\nIf you’ve never been here before, you are encouraged to work through the content on this site in order. Once you have an understanding of the concepts and workflows presented, we hope this site can serve as a reference you can revisit when you forget a step or get stuck in your process."
  },
  {
    "objectID": "index.html#analysis-and-coding-principles",
    "href": "index.html#analysis-and-coding-principles",
    "title": "Welcome to Tidy Movement Modeling with R",
    "section": "Analysis and Coding Principles",
    "text": "Analysis and Coding Principles\nAs with anything in science and R, there are a number of right ways to approach a problem. The workflows and principles outlined here aren’t the only way to develop animal movement models in R. However, this content has been developed after years of working with researchers and troubleshooting common issues. For most users, following this guide will prove a successful endeavor. More advanced users or those with specific needs should feel free to refer to this as a starting point but then expand to meet their need.\n\nSource Data are Read Only\nSource data should not be edited. In many cases, the financial and personal investment required to obtain these data is significant. In addition, the unique timing, location, and nature of the data cannot be replicated so every effort should be employed to preserve the integrity of source data as they were collected. All efforts should be made to also insure that source data are stored in plain text or well described open formats. This approach provides researchers confidence that data will be available and usable well into the future.\n\n\nScript Everything\nIn all likelihood, the format of any source data will not be conducive to established analytical procedures. For example, crawl cannot accept a raw data file downloaded from ArgosWeb or the Wildlife Computers Data Portal. Some amount of data processing is required. Keeping with the principles of reproducibility, all data assembly and pipelines should be scripted. Here, we rely on the R programming language for our scripts, but Python or other similar languages will also meet this need.\n\n\nDocument Along the Way\nScripting the data assembly should be combined with an effort to properly document the process. Documentation is an important component of reproducibility and, if done as part of the scripting and development process, provides an easier workflow for publishing results and data to journals and repositories. The rmarkdown, bookdown, and quarto packages provide an excellent framework for combining your scripts with documentation. This entire website is written with quarto and knitr.\n\n\nEmbrace the Tidyverse\nThe tidyverse describes a set of R packages developed around core principles of tidy data. Tidy data principles are outlined in a 2014 paper published in the Journal of Statistical Software. The key tenants of tidy data are:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nLocation data from telemetry deployments often follows this type of structure — each location estimate is a row and the columns all represent variable attributes associated with that location. Additionally, the location data are usually organized into a single table. Behavior data, however, often comes in a less structured format. Different behavior type data are sometimes mixed within a single table and column headings do not consistently describe the same variable (e.g. Bin1, Bin2, etc). There are valid reasons for tag vendors and data providers to transmit the source data in this way, but the structure is not conducive to analysis.\nThe tidyverse package is a wrapper for a number of separate packages that each contribute to the tidy’ing of data. The tidyr, dplyr, readr, and purrr packages are key components. In addition, the lubridate package (not included in the tidyverse package) is especially useful for consistent manipulation of date-time values. More information and documentation for the packages can be found at the tidyverse.org website.\nPlease note, however, that use of the tidyverse packages are not a requirement or indication of tidy data principles. So called, base R and other approaches in the R community can support tidy data and extensive use of tidyverse packages can also result in very un-tidy data.\n\n\nAnticipate Errors & Trap Them\nUse R’s error trapping functions (try() and tryCatch()) in areas of your code where you anticipate parsing errors or potential issues with convergence or fit. The purrr::safely() function also provides a great service."
  },
  {
    "objectID": "land-polygon.html",
    "href": "land-polygon.html",
    "title": "Sourcing a Land (barrier) Polygon",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "reading-data.html",
    "href": "reading-data.html",
    "title": "Reading Source data",
    "section": "",
    "text": "Our first step will be to read our source data into R. For this example, we’ll be using data for six bearded seal deployments of the northwest coast of Alaska. The data were downloaded as comma-separated files from the Wildlife Computers Data Portal. All data have merged into a single csv file and grouped by a ‘DeployID’ unique identifier. The raw-data/akbs_locs.csv contains all of the Argos location estimates determined for each deployment as well as fastloc GPS location estimates when available. At a minimum, this file includes identifying columns such as DeployID, PTT, Date, Quality, Type, Latitude, and Longitude. If the Argos Kalman filtering approach has been enabled (which it should be for any modern tag deployment), then additional data will be found in columns that describe the error ellipses (e.g. Error Semi-major axis, Error Semi-minor axis, Error Ellipse orientation). Note, if the tag transmitted GPS/FastLoc data, this file will list those records as Type = 'FastGPS'.\nIn many situations, the original source data may include multiple _*.csv_ or other delimited files. Instead of reading each file in one by one, the purr::map_df() function can be used to cycle through each _*.csv_ files in a directory and pass them to readr::read_csv() and, then, return a merged tibble/data.frame. This was the original case for these Alaska bearded seal data and the following code was used to create the combined _*.csv_ file from several _*-Locations.csv_ files.\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(here)\n\nmy_cols <- cols(\n  DeployID = col_character(),\n  Ptt = col_integer(),\n  Instr = col_character(),\n  Date = col_datetime(\"%H:%M:%S %d-%b-%Y\"),\n  Type = col_character(),\n  Quality = col_character(),\n  Latitude = col_double(),\n  Longitude = col_double(),\n  `Error radius` = col_integer(),\n  `Error Semi-major axis` = col_integer(),\n  `Error Semi-minor axis` = col_integer(),\n  `Error Ellipse orientation` = col_integer(),\n  Offset = col_character(),\n  `Offset orientation` = col_character(),\n  `GPE MSD` = col_character(),\n  `GPE U` = col_character(),\n  Count = col_character(),\n  Comment = col_character()\n)\n\ntbl_locs <- list.files(file.path('~/Downloads/akbs_data'),\n                       recursive = TRUE, \n                       full.names = TRUE, \n                       pattern = \"*-Locations.csv\") %>% \n  purrr::map_df( ~ readr::read_csv(.x, col_types = my_cols)) %>% \n  readr::write_csv(file = here::here('raw-data','akbs_locs.csv'))\n\nThe readr package includes the read_csv() function which we will rely on to read the csv data into R. This function is very similar to read.csv() but provides some additional functionality and consistency. One important step is to specify the column types for each column in the data set. This saves an additional step post-read where we have to mutate each column type. Note, we also rely on the here package for reliable and consistent file paths. Lastly, the janitor::clean_names() function is used to provide a consistent transformation of column names (e.g. lower and snake case with no spaces)."
  },
  {
    "objectID": "reading-data.html#efficient-data-storage-with-arrow",
    "href": "reading-data.html#efficient-data-storage-with-arrow",
    "title": "Reading Source data",
    "section": "Efficient Data Storage with Arrow",
    "text": "Efficient Data Storage with Arrow\nAs previously mentioned, thoughtful data management is an important component of any movement modeling analysis. For some, a central relational database (e.g., PostgreSQL) will be a good solution. However, in many cases, an organized collection of _*.csv_ files or an in-memory database solution (e.g. SQLite, DuckDb) will be a good option. For this example, we’re going to rely on the Apache arrow package for R and a collection of parquet files as our efficient data source.\n\nlibrary(arrow)\nlibrary(fs)\ndir_out <- here::here(\"data\",\"akbs_locs_parquet\")\nfs::dir_delete(dir_out)\nfs::dir_create(dir_out)\narrow::write_dataset(akbs_locs, dir_out, partitioning = \"deploy_id\")\nrm(akbs_locs) #remove from memory\n\nIf we take a quick look at the director/file structure withing our output directory, data/akbs_locs_parquet, we can see that arrow created a separate subdirectory for each deploy_id and there’s a separate _*.parquet_ file within each of those subdirectories\n\ndir_ls(dir_out) %>% path_rel(here::here(\"data\"))\n\nakbs_locs_parquet/deploy_id=EB2009_3000_06A1346\nakbs_locs_parquet/deploy_id=EB2009_3001_06A1332\nakbs_locs_parquet/deploy_id=EB2009_3002_06A1357\nakbs_locs_parquet/deploy_id=EB2011_3000_10A0219\nakbs_locs_parquet/deploy_id=EB2011_3001_10A0552\nakbs_locs_parquet/deploy_id=EB2011_3002_10A0200\n\n\n\ndir_ls(dir_out, recurse = TRUE, glob = \"*.parquet\") %>% \n  path_rel(here::here(\"data\"))\n\nakbs_locs_parquet/deploy_id=EB2009_3000_06A1346/part-0.parquet\nakbs_locs_parquet/deploy_id=EB2009_3001_06A1332/part-0.parquet\nakbs_locs_parquet/deploy_id=EB2009_3002_06A1357/part-0.parquet\nakbs_locs_parquet/deploy_id=EB2011_3000_10A0219/part-0.parquet\nakbs_locs_parquet/deploy_id=EB2011_3001_10A0552/part-0.parquet\nakbs_locs_parquet/deploy_id=EB2011_3002_10A0200/part-0.parquet"
  },
  {
    "objectID": "reading-data.html#querying-our-data",
    "href": "reading-data.html#querying-our-data",
    "title": "Reading Source data",
    "section": "Querying Our Data",
    "text": "Querying Our Data\nNow that we have our data organized with arrow and parquet files, we can explore ways the data can be queried and used in our analysis. There are two approaches and package frameworks we can rely on for querying:\n\nThe dplyr package’s support for arrow/parquet\nDirect use of SQL syntax with duckdb’s arrow/parquet integration\n\n\nA dplyr example\nTwo common tasks we might want to perform are to calculate the number of location records by deployment and to filter our data set based on a particular date range.\nIn this first example we create an open connection with our data set using the arrow::open_dataset() function and passing our dir_out path. Then, basic dplyr functions can return a table of location counts summarized by deploy_id. Note, unlike typical dplyr usage, the collect() function is required to actually return the data set records.\n\n# open the dataset\nakbs_ds <- arrow::open_dataset(dir_out) \n\n# query the dataset to calculate number of locations by deploy_id\nakbs_ds %>% \n  count(deploy_id, name=\"n_locs\") %>% \n  arrange(deploy_id) %>% \n  collect()\n\n# A tibble: 6 × 2\n  deploy_id           n_locs\n  <chr>                <int>\n1 EB2009_3000_06A1346   6491\n2 EB2009_3001_06A1332   5660\n3 EB2009_3002_06A1357   4913\n4 EB2011_3000_10A0219   9698\n5 EB2011_3001_10A0552   7559\n6 EB2011_3002_10A0200  10197\n\n\nIn this situation, we want to just look at locations from the months of August, September, and October. For this, the month() function from the lubridate package provides a way for us to filter by month integer.\n\n# query the data set to only include the months of August, \n# September, and October\nakbs_ds %>% \n  filter(month(date) %in% c(8,9,10)) %>% \n  relocate(deploy_id) %>% \n  collect()\n\n# A tibble: 17,597 × 18\n   deploy_id      ptt instr date                type  quality latitude longitude\n * <chr>        <dbl> <chr> <dttm>              <chr> <chr>      <dbl>     <dbl>\n 1 EB2009_3001… 74626 Mk10  2009-08-01 00:03:13 Fast… 9           70.8     -147.\n 2 EB2009_3001… 74626 Mk10  2009-08-01 00:07:15 Argos B           70.8     -147.\n 3 EB2009_3001… 74626 Mk10  2009-08-01 00:11:39 Argos B           70.8     -147.\n 4 EB2009_3001… 74626 Mk10  2009-08-01 00:17:06 Argos A           70.8     -147.\n 5 EB2009_3001… 74626 Mk10  2009-08-01 00:53:26 Argos B           70.8     -147.\n 6 EB2009_3001… 74626 Mk10  2009-08-01 01:46:52 Argos A           70.8     -147.\n 7 EB2009_3001… 74626 Mk10  2009-08-01 01:55:08 Argos A           70.8     -147.\n 8 EB2009_3001… 74626 Mk10  2009-08-01 02:26:28 Argos B           70.8     -147.\n 9 EB2009_3001… 74626 Mk10  2009-08-01 03:00:20 Argos A           70.8     -147.\n10 EB2009_3001… 74626 Mk10  2009-08-01 03:28:49 Argos B           70.7     -147.\n# … with 17,587 more rows, and 10 more variables: error_radius <dbl>,\n#   error_semi_major_axis <dbl>, error_semi_minor_axis <dbl>,\n#   error_ellipse_orientation <dbl>, offset <lgl>, offset_orientation <lgl>,\n#   gpe_msd <lgl>, gpe_u <lgl>, count <lgl>, comment <chr>\n\n\n\n\nA duckdb example\nDuckDB is a powerful in-process database management system that is easily installed as an R package and provides built-in support for our arrow/parquet data. With the duckdb package as well as DBI, we can pass standard SQL code to query our data set. This is especially useful if you are familiar/comfortable with SQL and you want to develop some fairly complex queries for your data.\n\nlibrary(duckdb)\n# open connection to DuckDB\ncon <- dbConnect(duckdb::duckdb())\n\n# register the data set as a DuckDB table, and give it a name\nduckdb::duckdb_register_arrow(con, \"akbs_locs\", akbs_ds)\n\n# query\nres <- dbGetQuery(con, \n           \"SELECT deploy_id, COUNT(*) AS n_locs \n           FROM akbs_locs \n           GROUP BY deploy_id\")\n\n# clean up\nduckdb_unregister(con, \"akbs_locs\")\ndbDisconnect(con, shutdown = TRUE)\n\ntibble(res)\n\n# A tibble: 6 × 2\n  deploy_id           n_locs\n  <chr>                <dbl>\n1 EB2009_3000_06A1346   6491\n2 EB2011_3000_10A0219   9698\n3 EB2009_3001_06A1332   5660\n4 EB2011_3001_10A0552   7559\n5 EB2009_3002_06A1357   4913\n6 EB2011_3002_10A0200  10197\n\n\nAn example of a more complex SQL query that might be of interest is to identify records with duplicate date-time columns within each deployment. We can do this with DuckDB and SQL.\n\ncon <- dbConnect(duckdb::duckdb())\n\n# register the data set as a DuckDB table, and give it a name\nduckdb::duckdb_register_arrow(con, \"akbs_locs\", akbs_ds)\n\n# query\nres <- dbGetQuery(con, \n           \"SELECT a.deploy_id, a.date, a.quality,\n           a.latitude, a.longitude, a.error_radius\n            FROM akbs_locs a\nJOIN (SELECT deploy_id, date, COUNT(*),\nFROM akbs_locs \nGROUP BY deploy_id, date\nHAVING count(*) > 1 ) b\nON a.deploy_id = b.deploy_id AND\na.date = b.date\nORDER BY a.deploy_id, a.date\")\n\n# clean up\nduckdb_unregister(con, \"akbs_locs\")\ndbDisconnect(con, shutdown = TRUE)\n\ntibble(res)\n\n# A tibble: 4,820 × 6\n   deploy_id         date                quality latitude longitude error_radius\n   <chr>             <dttm>              <chr>      <dbl>     <dbl>        <dbl>\n 1 EB2009_3000_06A1… 2009-06-25 02:21:37 A           66.5     -163.          210\n 2 EB2009_3000_06A1… 2009-06-25 02:21:37 A           66.5     -163.          461\n 3 EB2009_3000_06A1… 2009-06-28 01:10:04 B           66.7     -163.         2154\n 4 EB2009_3000_06A1… 2009-06-28 01:10:04 B           66.8     -163.          661\n 5 EB2009_3000_06A1… 2009-06-30 01:24:03 B           67.6     -164.         5687\n 6 EB2009_3000_06A1… 2009-06-30 01:24:03 B           67.6     -164.          398\n 7 EB2009_3000_06A1… 2009-07-02 09:34:28 B           69.1     -166.         1736\n 8 EB2009_3000_06A1… 2009-07-02 09:34:28 B           69.1     -166.         2475\n 9 EB2009_3000_06A1… 2009-07-02 15:54:47 B           69.2     -166.         1246\n10 EB2009_3000_06A1… 2009-07-02 15:54:47 B           69.2     -166.          522\n# … with 4,810 more rows\n\n\nIf we look at the above table, we can see the duplicate date column values but also notice the coordinate values are different. Instead of just removing one of the duplicates at random, we can use complex SQL to retain the duplicate value with the lower error_radius.\n\ncon <- dbConnect(duckdb::duckdb())\n\n# register the data set as a DuckDB table, and give it a name\nduckdb::duckdb_register_arrow(con, \"akbs_locs\", akbs_ds)\n\n# query\nres <- dbGetQuery(con,\n           \"select a.*\n            from\n            (select *\n            ,ROW_NUMBER() over (\n            partition by deploy_id, date\n            order by error_radius ASC) rn \n            from akbs_locs) a\n            where a.rn = 1\n            order by deploy_id, date, error_radius\")\n\n# clean up\nduckdb_unregister(con, \"akbs_locs\")\ndbDisconnect(con, shutdown = TRUE)\n\ntibble(res)\n\n# A tibble: 42,047 × 19\n     ptt instr date                type  quality latitude longitude error_radius\n   <dbl> <chr> <dttm>              <chr> <chr>      <dbl>     <dbl>        <dbl>\n 1 74627 Mk10  2009-06-23 00:00:00 User  <NA>        66.4     -162.           NA\n 2 74627 Mk10  2009-06-23 02:41:12 Argos A           66.3     -163.          476\n 3 74627 Mk10  2009-06-23 03:07:11 Argos B           66.4     -162.          658\n 4 74627 Mk10  2009-06-23 04:08:10 Argos B           66.3     -162.         5178\n 5 74627 Mk10  2009-06-23 04:15:36 Argos B           66.3     -162.         3877\n 6 74627 Mk10  2009-06-23 04:27:12 Argos A           66.3     -162.          157\n 7 74627 Mk10  2009-06-23 04:44:54 Argos B           66.3     -162.          432\n 8 74627 Mk10  2009-06-23 05:56:58 Argos A           66.3     -163.          174\n 9 74627 Mk10  2009-06-23 06:24:33 Argos A           66.4     -162.          262\n10 74627 Mk10  2009-06-23 07:35:42 Argos B           66.4     -162.          972\n# … with 42,037 more rows, and 11 more variables: error_semi_major_axis <dbl>,\n#   error_semi_minor_axis <dbl>, error_ellipse_orientation <dbl>, offset <lgl>,\n#   offset_orientation <lgl>, gpe_msd <lgl>, gpe_u <lgl>, count <lgl>,\n#   comment <chr>, deploy_id <chr>, rn <dbl>"
  },
  {
    "objectID": "reading-data.html#create-our-location-dataset",
    "href": "reading-data.html#create-our-location-dataset",
    "title": "Reading Source data",
    "section": "Create Our Location Dataset",
    "text": "Create Our Location Dataset\nWe’re going to rely on the typical dplyr approach to query our source data and distill it down to only the essential columns needed for our analysis. Also, since we have a combination of Argos and FastGPS data, we need to create a separate column for the number of satellites used during the fastloc GPS calculation.\n\nakbs_ds <- arrow::open_dataset(dir_out)\n\nakbs_locs <- akbs_ds %>% \n  dplyr::select(deploy_id,\n                date,\n                type,\n                quality,\n                latitude,\n                longitude,\n                error_radius,\n                error_semi_major_axis,\n                error_semi_minor_axis,\n                error_ellipse_orientation) %>% \n  mutate(\n    num_sats = case_when(\n      type == \"FastGPS\" ~ quality,\n      TRUE ~ NA_character_\n      ),\n    quality = case_when(\n      type == \"FastGPS\" ~ NA_character_,\n      TRUE ~ quality\n    ),\n    num_sats = as.integer(num_sats)\n    ) %>% \n  collect()"
  },
  {
    "objectID": "reading-data.html#remove-duplicate-date-time-records",
    "href": "reading-data.html#remove-duplicate-date-time-records",
    "title": "Reading Source data",
    "section": "Remove Duplicate Date-Time Records",
    "text": "Remove Duplicate Date-Time Records\nIt’s not unusual for a small number of records within a deployment to have duplicate times. We will want to identify and remove thiese records early in the process.\n\nakbs_locs <- akbs_locs %>% \n  group_by(deploy_id) %>% \n  arrange(date, error_radius) %>% \n  mutate(\n    rank = 1L,\n    rank = case_when(duplicated(date, fromLast = FALSE) ~\n                              lag(rank) + 1L, TRUE ~ rank)) %>% \n  dplyr::filter(rank == 1)"
  },
  {
    "objectID": "reading-data.html#convert-to-spatial-feature",
    "href": "reading-data.html#convert-to-spatial-feature",
    "title": "Reading Source data",
    "section": "Convert to Spatial Feature",
    "text": "Convert to Spatial Feature\nSince our data are, at the core, spatial observations we will benefit by converting our akbs_locs tibble/data.frame into a spatial point feature with the sfpackage. In order to accomplish this, we need to identify our coordinate columns and know the coordinate reference system (CRS). For all Argos and GPS data, the coordinate reference system is geographic with X and Y represented as longitude (-180, 180) and latitude (-90, 90). You should ensure the data are formatted as decimal degrees and not some combination of degrees, minutes, and seconds. To specify the CRS, we’ll rely on the designated EPSG code of 4326 which tells sf that our data are geographic.\n\nlibrary(sf)\n\nakbs_locs <- sf::st_as_sf(akbs_locs, coords = c(\"longitude\",\"latitude\"),\n                          crs = \"epsg:4326\")\n\nakbs_locs %>% \n  dplyr::select(deploy_id, date, geometry)\n\nSimple feature collection with 42047 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -175.137 ymin: -15.7054 xmax: 170.878 ymax: 73.478\nGeodetic CRS:  WGS 84\n# A tibble: 42,047 × 3\n# Groups:   deploy_id [6]\n   deploy_id           date                           geometry\n   <chr>               <dttm>                      <POINT [°]>\n 1 EB2009_3000_06A1346 2009-06-23 00:00:00     (-162.42 66.38)\n 2 EB2009_3000_06A1346 2009-06-23 02:41:12 (-162.5562 66.3168)\n 3 EB2009_3000_06A1346 2009-06-23 03:07:11 (-162.4431 66.3656)\n 4 EB2009_3000_06A1346 2009-06-23 04:08:10 (-162.3752 66.3216)\n 5 EB2009_3000_06A1346 2009-06-23 04:15:36 (-162.3757 66.3222)\n 6 EB2009_3000_06A1346 2009-06-23 04:27:12  (-162.3922 66.345)\n 7 EB2009_3000_06A1346 2009-06-23 04:44:54 (-162.4411 66.3316)\n 8 EB2009_3000_06A1346 2009-06-23 05:56:58 (-162.5032 66.3487)\n 9 EB2009_3000_06A1346 2009-06-23 06:24:33 (-162.4993 66.3518)\n10 EB2009_3000_06A1346 2009-06-23 07:35:42 (-162.4958 66.3583)\n# … with 42,037 more rows\n\n\nWe can see that instead of separate columns for longitude and latitude we now have a single geometry column that describes the point geometry of each location. Also note that the metadata indicates the Geodetic CRS is specified as WGS 84 which tells is our CRS specification is correctly set to geographic with longitude/latitude coordinate values."
  },
  {
    "objectID": "crawl-utils.html",
    "href": "crawl-utils.html",
    "title": "Easier crawl-ing with crawlUtils",
    "section": "",
    "text": "In recent months, the crawlUtils package has been developed to assist with and streamline the most common pipelines for modeling animal movement with the crawl and pathroutr packages. These approaches won’t work for everyone in every situation. But, they are good starting points and, hopefully, allow you to proceed quickly from a tidy data set of observations to a fitted model with predicted tracks and locations."
  },
  {
    "objectID": "crawl-utils.html#prepare-data-for-crawlutils",
    "href": "crawl-utils.html#prepare-data-for-crawlutils",
    "title": "Easier crawl-ing with crawlUtils",
    "section": "Prepare Data for crawlUtils",
    "text": "Prepare Data for crawlUtils\nWe’re going to start with our sf simple feature collection that has been processed with the speed, distance, and angle filter. While exploring and mapping the observations, the crsuggest package was used to determine an optimal coordinate reference system for our data. In the end, you know your data and your region the best and this knowledge should form the basis of your decision regarding the CRS you will use.\nSince we’ve already explored our data with maps using the top CRS suggested, we’ll continue with that\n\nakbs_epsg <- crsuggest::suggest_top_crs(akbs_locs)\n\nakbs_locs <- akbs_locs %>% \n  sf::st_transform(akbs_epsg)\n\nThe crwUtils package has some expectations regarding the column names that we’ll need to adjust in our data. Namely, we need to include the number of satellites for fastLoc GPS observations within the quality column and our date column needs to be renamed to datetime.\nThere are also some additional columns required so that multiple source data types (FastGPS, Argos least-squares, and Argos Kalman filter) can be used simultaneously within the same deployment. This is handled for us by simply calling the cu_add_argos_cols() function. Note, use of this function does require the following columns with exact names:\n\ntype, indicate the type of location,\nquality which indicates the Argos quality of the location,\nthe Argos KF diagnostics: error_semi_major_axis_, error_semi_minor_axis, and error_ellipse_orientation\n\nBecause our data set already has these columns, there’s no additional edits needed and we can just call the function with no arguments.\n\nakbs_locs <- akbs_locs %>% \n  mutate(\n    quality = case_when(\n      type == \"FastGPS\" ~ as.character(num_sats),\n      TRUE ~ quality\n    ),\n    type = case_when(\n      type == \"User\" ~ \"known\",\n      TRUE ~ type\n    )) %>% \n  rename(\n    datetime = date \n  ) %>% \n  cu_add_argos_cols()"
  },
  {
    "objectID": "crawl-utils.html#fit-basic-ctcrw-model",
    "href": "crawl-utils.html#fit-basic-ctcrw-model",
    "title": "Easier crawl-ing with crawlUtils",
    "section": "Fit Basic CTCRW Model",
    "text": "Fit Basic CTCRW Model\nWe’re now ready to, finally, fit our CTCRW model with crawl. Because we have multiple deployments that can be fit independently, we need to make one final enhancement to our tidy data structure — created a nested tibble. Nesting takes advantage of list columns in R and allows us to store data objects within our tibble. In this case, we’ll create a single row for each deployment and our data will be stored within a special data column. As we progress through the fitting process, we’ll store results such as model fits and predictions as additional columns.\n\nakbs_locs <- group_by(akbs_locs, deploy_id) %>% \n  tidyr::nest()\n\nNow, we can proceed to fit a CTCRW movement model for each of our deployments. The results of each fit will be stored in the fit column. Previous examples for this step often required careful attention and customization of various model parameters. crawlUtils aims to provide a generalized solution that should work for most scenarios.\n\nakbs_locs <- akbs_locs %>%\n  mutate(\n    fit = cu_crw_argos(data)\n  )\n\nWhile fitting the model, you may see a warning that indicates: > Warning: Animal 1 has both LS and KF Argos location types or other unknown types! > Keeping KF becuase KF represents the larger percentage of the observations.\nThis is because, currently, it’s not possible to accommodate both the Kalman Filter (KF) error and the Least Squares (LS) quality error (e.g. A, B, 0, 1, 2, 3). The function will examine your data and keep the error that is used across the majority of locations in that deployment. It is possible (preferable) to have both KF or LS and FastGPS locations within the same deployment.\nLet’s take a quick look at the first model fit to make sure there aren’t any red flags in the result. If you want to look at all the fits with one call you can eliminate the pluck(1) portion in the code below.\n\nakbs_locs %>% pull(fit) %>% pluck(1)\n\n\n\nContinuous-Time Correlated Random Walk fit\n\nModels:\n--------\nMovement   ~ 1\nError   ~0 + ln.sd.x ~0 + ln.sd.y ~error.corr\n\n                     Parameter Est. St. Err. 95% Lower 95% Upper\nln tau.x ln.sd.x              1.000        .         .         .\nln tau.y ln.sd.y              1.000        .         .         .\nln sigma (Intercept)          7.652    0.014     7.624      7.68\nln beta (Intercept)           2.220    0.176     1.876     2.564\n\n\nLog Likelihood = -101576.783 \nAIC = 203157.565"
  },
  {
    "objectID": "crawl-utils.html#predict-locations",
    "href": "crawl-utils.html#predict-locations",
    "title": "Easier crawl-ing with crawlUtils",
    "section": "Predict Locations",
    "text": "Predict Locations\nWith our model fits in hand, we can use that model to predict locations at regular (or otherwise specified) intervals. These predictions will form the basis of our subsequent analyses. And, as such, you should consider the prediction interval carefully. The continuous nature of crawl’s approach means you can always come back and predict again at a different interval without having to re-fit the model.\nThere are some built in parsers for time so you can say, for example predTime=\"1 hours\" or predTime=\"6 hours\" or predTime=\"20 minutes\" and the prediction function will generate the intervals for you. You can also provide a custom vector of prediction times. Note that we specify as_sf=TRUE so an sf point object is returned.\n\nakbs_locs <- akbs_locs %>%\n  mutate(\n    pred = cu_crw_predict(fit, predTime=\"1 hours\", as_sf=TRUE)\n  )\n\nNow that we have our predictions, let’s revisit one of our previous maps and, this time, create a path based on the hourly predictions instead of the raw observations.\nThe first thing we need to do is extract the predictions from our nested tibble. We can accomplish this with the tidyr::unnest() function and, then, filter the resulting data to only include predicted locations (locType == \"p\"). The sf geometry data is preserved but we need just do a quick st_as_sf() call to reestablish our data as and sf object. Lastly, as before, we’ll want to convert the predicted point data into lines.\n\nlibrary(stringr)\nakbs_preds <- akbs_locs %>% \n  tidyr::unnest(cols=pred) %>% \n  dplyr::filter(locType == \"p\") %>% \n  dplyr::select(deploy_id, datetime, geometry) %>% \n  sf::st_as_sf() %>% \n  dplyr::arrange(datetime) %>% \n  dplyr::mutate(deploy_id = stringr::str_sub(deploy_id, 1,11)) %>% \n  dplyr::summarise(do_union = FALSE) %>% \n  sf::st_cast(\"MULTILINESTRING\")\n\n\nlibrary(colorspace)\nlibrary(ggplot2)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\nworld <- ne_countries(scale = \"medium\",returnclass = \"sf\") %>% \n  sf::st_make_valid() %>% \n  sf::st_transform(akbs_epsg)\n\nmap <- ggplot() + geom_sf(data = world) +\n  geom_sf(data = akbs_preds, aes(color = deploy_id),\n          lwd = 0.5) + \n  coord_sf(\n    xlim = sf::st_bbox(akbs_preds)[c(1,3)],\n    ylim = sf::st_bbox(akbs_preds)[c(2,4)]) +\n  scale_color_discrete_qualitative(\n    palette = \"Dark 3\",\n    name = \"deployment\") +\n  labs(title = \"Predicted Movement Paths of 6 Bearded Seals in NW Alaska\",\n        subtitle = paste0(\"paths based on hourly predictions\")) +\n  theme_minimal()\n\nmap"
  },
  {
    "objectID": "crawl-utils.html#multiple-imputation-for-uncertainty",
    "href": "crawl-utils.html#multiple-imputation-for-uncertainty",
    "title": "Easier crawl-ing with crawlUtils",
    "section": "Multiple Imputation for Uncertainty",
    "text": "Multiple Imputation for Uncertainty\nThe predicted path represents the most likely movement for the animal given the observed locations and their reported error. A single line, however, doesn’t fully communicate the uncertainty associated with each prediction. To better demonstrate this, we’ll rely on multiple imputation from our model fit to create a sample of additionally possible tracks for each deployment. These tracks along with the predicted track should communicate a better picture of where the animals might have traveled during the deployment.\nThe crawlUtils package provides a helper function, cu_crw_sample, that handles creation of tracks via multiple imputation. All we need to provide are the number of simulated tracks we’d like to generate and the desired prediction interval. To keep things from exploding computationally, we’ll set the number of generated tracks at 8 and keep our prediction time to 1 hour.\n\nakbs_locs <- akbs_locs %>%\n  mutate(\n    sims = cu_crw_sample(size=8, fit, predTime=\"1 hours\", as_sf=TRUE)\n  )\n\nAs before, we need to extract the pertinent spatial information from our nested tibble. In this case, each of our 8 simulated tracks are stored within a list of sf objects in the sims column. We need to unnest the sims column like before but, also, combine the 8 separate simulated tracks into a single object.\nOh, and then don’t forget to convert each of the simulations into lines!\n\nakbs_sims <- akbs_locs %>% \n  rowwise() %>% \n  mutate(sims = list(bind_rows(sims, .id = \"sim_id\"))) %>% \n  unnest(cols=sims) %>% \n  dplyr::select(deploy_id, sim_id, datetime, geometry) %>% \n  st_as_sf() %>% \n  group_by(deploy_id,sim_id) %>% \n  dplyr::arrange(datetime) %>% \n  dplyr::mutate(deploy_id = stringr::str_sub(deploy_id, 1,11)) %>% \n  dplyr::summarise(do_union = FALSE) %>% \n  sf::st_cast(\"MULTILINESTRING\")\n\nNow, we can plot all of these additional lines along with our original predicted tracks. We can build on our previous plot and simply add another layer to plot the predicted tracks but at a slightly smaller size and with some alpha transparency.\n\nworld <- ne_countries(scale = \"medium\",returnclass = \"sf\") %>% \n  sf::st_make_valid() %>% \n  sf::st_transform(akbs_epsg)\n\nmap <- ggplot() + geom_sf(data = world) +\n  geom_sf(data = akbs_sims, aes(color = deploy_id), alpha = 0.1, size=0.5) +\n  geom_sf(data = akbs_preds, aes(color = deploy_id),\n          size = 0.3) + \n  coord_sf(\n    xlim = sf::st_bbox(akbs_sims)[c(1,3)],\n    ylim = sf::st_bbox(akbs_sims)[c(2,4)]) +\n  scale_color_discrete_qualitative(\n    palette = \"Dark 3\",\n    name = \"deployment\") +\n  labs(title = \"Predicted Movement of 6 Bearded Seals in NW Alaska\",\n        subtitle = paste0(\"most likely path and imputed paths shown\")) +\n  theme_minimal()\n\nmap"
  },
  {
    "objectID": "about-authors.html",
    "href": "about-authors.html",
    "title": "About the Authors",
    "section": "",
    "text": "Drs. Josh M. London and Devin S. Johnson are researchers with the NOAA National Marine Fisheries Service. Dr. London is a wildlife biologist at the Alaska Fisheries Science Center’s Marine Mammal Laboratory in Seattle, Washington. Dr. Johnson is a mathematical statistician at the Pacific Islands Fisheries Science Center’s Protected Species Division in Honolulu, Hawaii. Dr. London has over 15 years of experience programming and deploying satellite tags on phocid seals. He has also developed various workflows for the management of telemetry data in R. Dr. Johnson is a leading statistical ecologist with expertise in the analysis of animal movement. Dr. Johnson is the lead author and developer of the R package crawl. Dr. London is the lead author of the R package pathroutr and co-developer of crawl."
  },
  {
    "objectID": "behavior-plots.html",
    "href": "behavior-plots.html",
    "title": "Exploring Animal Behavior",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "pathroutr-demo.html",
    "href": "pathroutr-demo.html",
    "title": "A pathroutr Harbor Seal Example",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "intro-to-pathroutr.html",
    "href": "intro-to-pathroutr.html",
    "title": "Introduction to the pathroutr Package",
    "section": "",
    "text": "This is a Quarto website"
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "Package Installation & Dependencies",
    "section": "",
    "text": "The examples and workflows described here are all developed with R version 4.2 and you are strongly encouraged to update your version to 4.2 or greater. In general, the code and examples provided should work reliably with any version of R greater than 4.0."
  },
  {
    "objectID": "packages.html#install-core-packages",
    "href": "packages.html#install-core-packages",
    "title": "Package Installation & Dependencies",
    "section": "Install Core Packages",
    "text": "Install Core Packages\nThe focus of this site, and the example workflows we describe, is on animal movement modeling in R using the {crawl} package. In addition, the recently developed {pathroutr} and {crawlUtils} packages are key enhancements that extend the capabilities of {crawl} and improve the user experience.\nYou are strongly encouraged to install the latest available versions of each package.\n\nInstall {crawl}\n\nInstall via CRAN\n{crawl} is currently available on CRAN and R >= 4.0 is highly recommended.\n\n# install latest version of crawl from CRAN\ninstall.packages(\"crawl\")\n\n\n\nInstall via R-Universe\nThe latest version of {crawl} is also available via R-Universe.\n\n# Install crawl from my R-Universe repository\n# Enable repository from dsjohnson\noptions(repos = c(\n  dsjohnson = 'https://dsjohnson.r-universe.dev',\n  CRAN = 'https://cloud.r-project.org'))\n# Download and install crawl in R\ninstall.packages('crawl')\n# Browse the crawl manual pages\nhelp(package = 'crawl')\n\nYou can also add the repository to your local list of repositories in your .Rprofile and this will ensure update.packages() pulls any new releases of {crawl} from R-Universe\n\n#install.packages(\"usethis\")\nusethis::edit_r_profile()\n# add the following text or replace existing repos option\noptions(repos = c(dsjohnson = 'https://dsjohnson.r-universe.dev',\n                  CRAN = 'https://cloud.r-project.org'))\n\n\n\nInstall via Github\nA development version of {pathroutr} is also available from GitHub. This version should be used with caution and only after consulting with package authors.\n\n# install.packages(\"remotes\")\nremotes::install_github(\"NMML/crawl@devel\")\n\n\n\n\nInstall {pathroutr}\n{pathroutr} is currently not available on CRAN and also requires R >= 4.0. Please upgrade your version of R, if needed, before proceeding. Future versions of {pathroutr} may support pre-4.0 versions of R. But, for now, only 4.0+ is supported.\n\nInstall via R-Universe\nStarting with v0.2.1, {pathroutr} is available via R-Universe.\n\n# Install new pathroutr version from my R-Universe repository\ninstall.packages(\"pathroutr\", repos = \"https://jmlondon.r-universe.dev\")\n\nYou can also add my repository to your local list of repositories in your .Rprofile and this will ensure update.packages() pulls any new releases of {pathroutr}\n\n#install.packages(\"usethis\")\nusethis::edit_r_profile()\n# add the following text or replace existing repos option\noptions(repos = c(jmlondon = 'https://jmlondon.r-universe.dev',\n                  CRAN = 'https://cloud.r-project.org'))\n\n\n\nInstall via Github\nThe development version of {pathroutr} is available from GitHub with:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"jmlondon/pathroutr\")\n\n\n\n\nInstall {crawlUtils}\n\nInstall via R-Universe\n\n# Install crawlUtils from Devin's R-Universe repository\noptions(repos = c(\n  dsjohnson = 'https://dsjohnson.r-universe.dev',\n  CRAN = 'https://cloud.r-project.org'))\n# Download and install crawlUtils in R\ninstall.packages('crawlUtils')\n# Browse the crawlUtils manual pages\nhelp(package = 'crawlUtils')\n\nYou can also add the repository to your local list of repositories in your .Rprofile and this will ensure update.packages() pulls any new releases of {crawlUtils} from R-Universe\n\n#install.packages(\"usethis\")\nusethis::edit_r_profile()\n# add the following text or replace existing repos option\noptions(repos = c(dsjohnson = 'https://dsjohnson.r-universe.dev',\n                  CRAN = 'https://cloud.r-project.org'))\n\n\n\nInstall via Github\nThe development version of {crawlUtils} is available from GitHub with:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"dsjohnson/crawlUtils\")"
  },
  {
    "objectID": "history-of-crawl.html",
    "href": "history-of-crawl.html",
    "title": "History of crawl and Movement Models in R",
    "section": "",
    "text": "This is a Quarto website"
  }
]